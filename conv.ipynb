{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Dependencies #\n",
    "################\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataset into smaller chunks\n",
    "def split_file(file_path, out_path, customers_per_file):\n",
    "    customers_per_file = customers_per_file + 1\n",
    "    ids = {}\n",
    "    cnt = 0\n",
    "    ids_file = open('{}/{}.csv'.format(out_path, \"ids\"), \"w\")\n",
    "    ids_file.write(\"id,chunk\\n\")\n",
    "    save = False\n",
    "    with open(file_path) as bigfile:\n",
    "        for lineno, line in enumerate(bigfile):\n",
    "            if lineno == 0: continue    # skip header\n",
    "            customer_ID = line.split(\",\")[0]    # get customer ID of current line\n",
    "\n",
    "            sum_ids = len(ids.keys())\n",
    "\n",
    "            # add customer to list of read customers\n",
    "            if customer_ID in ids: ids[customer_ID].append(line)\n",
    "            else:\n",
    "                if sum_ids != (customers_per_file - 1): ids[customer_ID] = [line]\n",
    "                else: save = True\n",
    "\n",
    "\n",
    "            # write data of all read customers to a new file\n",
    "            if save:\n",
    "                data_file = open(f\"{out_path}/data{cnt}.csv\", \"w\")\n",
    "                data_file.write(\"customer_ID,S_2,P_2,D_39,B_1,B_2,R_1,S_3,D_41,B_3,D_42,D_43,D_44,B_4,D_45,B_5,R_2,D_46,D_47,D_48,D_49,B_6,B_7,B_8,D_50,D_51,B_9,R_3,D_52,P_3,B_10,D_53,S_5,B_11,S_6,D_54,R_4,S_7,B_12,S_8,D_55,D_56,B_13,R_5,D_58,S_9,B_14,D_59,D_60,D_61,B_15,S_11,D_62,D_63,D_64,D_65,B_16,B_17,B_18,B_19,D_66,B_20,D_68,S_12,R_6,S_13,B_21,D_69,B_22,D_70,D_71,D_72,S_15,B_23,D_73,P_4,D_74,D_75,D_76,B_24,R_7,D_77,B_25,B_26,D_78,D_79,R_8,R_9,S_16,D_80,R_10,R_11,B_27,D_81,D_82,S_17,R_12,B_28,R_13,D_83,R_14,R_15,D_84,R_16,B_29,B_30,S_18,D_86,D_87,R_17,R_18,D_88,B_31,S_19,R_19,B_32,S_20,R_20,R_21,B_33,D_89,R_22,R_23,D_91,D_92,D_93,D_94,R_24,R_25,D_96,S_22,S_23,S_24,S_25,S_26,D_102,D_103,D_104,D_105,D_106,D_107,B_36,B_37,R_26,R_27,B_38,D_108,D_109,D_110,D_111,B_39,D_112,B_40,S_27,D_113,D_114,D_115,D_116,D_117,D_118,D_119,D_120,D_121,D_122,D_123,D_124,D_125,D_126,D_127,D_128,D_129,B_41,B_42,D_130,D_131,D_132,D_133,R_28,D_134,D_135,D_136,D_137,D_138,D_139,D_140,D_141,D_142,D_143,D_144,D_145\\n\")\n",
    "                \n",
    "                for key in ids:\n",
    "                    data = ids[key]\n",
    "                    # copy first element of series until series length is 13\n",
    "                    if len(data) != 13: data = [data[0] for cnt in range(13 - len(data) )] + data \n",
    "                    ids_file.write(key + \",\" + str(cnt) + \"\\n\")\n",
    "                    for elem in data: data_file.write(elem)\n",
    "\n",
    "                data_file.close()\n",
    "                ids = {}\n",
    "                ids[customer_ID] = [line]\n",
    "                save = False\n",
    "                cnt += 1\n",
    "\n",
    "    data_file = open(f\"{out_path}/data{cnt}.csv\", \"w\")\n",
    "    data_file.write(\"customer_ID,S_2,P_2,D_39,B_1,B_2,R_1,S_3,D_41,B_3,D_42,D_43,D_44,B_4,D_45,B_5,R_2,D_46,D_47,D_48,D_49,B_6,B_7,B_8,D_50,D_51,B_9,R_3,D_52,P_3,B_10,D_53,S_5,B_11,S_6,D_54,R_4,S_7,B_12,S_8,D_55,D_56,B_13,R_5,D_58,S_9,B_14,D_59,D_60,D_61,B_15,S_11,D_62,D_63,D_64,D_65,B_16,B_17,B_18,B_19,D_66,B_20,D_68,S_12,R_6,S_13,B_21,D_69,B_22,D_70,D_71,D_72,S_15,B_23,D_73,P_4,D_74,D_75,D_76,B_24,R_7,D_77,B_25,B_26,D_78,D_79,R_8,R_9,S_16,D_80,R_10,R_11,B_27,D_81,D_82,S_17,R_12,B_28,R_13,D_83,R_14,R_15,D_84,R_16,B_29,B_30,S_18,D_86,D_87,R_17,R_18,D_88,B_31,S_19,R_19,B_32,S_20,R_20,R_21,B_33,D_89,R_22,R_23,D_91,D_92,D_93,D_94,R_24,R_25,D_96,S_22,S_23,S_24,S_25,S_26,D_102,D_103,D_104,D_105,D_106,D_107,B_36,B_37,R_26,R_27,B_38,D_108,D_109,D_110,D_111,B_39,D_112,B_40,S_27,D_113,D_114,D_115,D_116,D_117,D_118,D_119,D_120,D_121,D_122,D_123,D_124,D_125,D_126,D_127,D_128,D_129,B_41,B_42,D_130,D_131,D_132,D_133,R_28,D_134,D_135,D_136,D_137,D_138,D_139,D_140,D_141,D_142,D_143,D_144,D_145\\n\")\n",
    "                \n",
    "    for key in ids:\n",
    "        data = ids[key]\n",
    "        # copy first element of series until series length is 13\n",
    "        if len(data) != 13: data = [data[0] for cnt in range(13 - len(data) )] + data \n",
    "        ids_file.write(key + \",\" + str(cnt) + \"\\n\")\n",
    "        for elem in data: data_file.write(elem)\n",
    "\n",
    "    data_file.close()\n",
    "    ids = {}\n",
    "    cnt += 1\n",
    "\n",
    "    ids_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_per_file = 10000\n",
    "#split_file(\"./train_data/train_data.csv\",\"./train_data/train_data_chunks/\", customers_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmericanExpressProfileTimeSeriesDatasetLowMemory(Dataset):\n",
    "    def __init__(self, labels_file, data_folder, train=True, train_percentage=0.7, labels_available=True):\n",
    "        self.labels_available = labels_available\n",
    "        if self.labels_available: self.labels = pd.read_csv(labels_file)\n",
    "        self.data_folder = data_folder\n",
    "        self.chunk_indices = pd.read_csv(data_folder+\"/ids.csv\")  # maps from customer_ID to the number of the chunk which stores the date for the customer_ID\n",
    "        \n",
    "        self.start_index = 0 if train else math.ceil(len(self.chunk_indices) * train_percentage)\n",
    "        self.end_index = math.floor(len(self.chunk_indices) * train_percentage) if train else len(self.chunk_indices)\n",
    "\n",
    "        self.chunk_indices = self.chunk_indices.iloc[self.start_index:self.end_index]\n",
    "\n",
    "        self.current_chunk_index = 0\n",
    "        self.current_chunk_data = pd.read_csv(self.data_folder + \"/data\" +  str(self.current_chunk_index) + \".csv\")\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_indices)  # use self.chunk_indices instead of self.labels as some customer data with series length != 13 was deleted\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.chunk_indices.iloc[idx]\n",
    "        customer_ID = entry[\"id\"]\n",
    "        chunk_index = entry[\"chunk\"]\n",
    "\n",
    "        if chunk_index != self.current_chunk_index:\n",
    "            self.current_chunk_index = chunk_index\n",
    "            self.current_chunk_data = pd.read_csv(self.data_folder + \"/data\" + str(self.current_chunk_index) + \".csv\")\n",
    "\n",
    "        data = self.current_chunk_data[self.current_chunk_data[\"customer_ID\"] == customer_ID]   # is slow for large files => use small chunks\n",
    "        data = data.drop(['D_63', 'D_64', 'S_2', 'customer_ID'], axis=1)\n",
    "\n",
    "        # TODO: can the time value be used? maybe as unix time number\n",
    "        # TODO: create one hot encoding for missing 2 features\n",
    "\n",
    "        data = data.fillna(0)   # TODO: Is it valid to fill all missing values with 0?\n",
    "        data = data.values\n",
    "        data = torch.tensor(data, dtype=torch.float32, requires_grad=True).T\n",
    "        data = data.to(device)\n",
    "\n",
    "        if self.labels_available:\n",
    "            label = self.labels[self.labels[\"customer_ID\"] == customer_ID][\"target\"].values[0]\n",
    "            label = torch.tensor(label, dtype=torch.float32)\n",
    "            label = label.to(device)\n",
    "            return data, label\n",
    "        else: return data, customer_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AmericanExpressProfileTimeSeriesDatasetLowMemory(\"./train_data/train_labels.csv\", \"./train_data/train_data_chunks/\", True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64) # do not shuffle as this would make the data loader very slow\n",
    "\n",
    "test_dataset = AmericanExpressProfileTimeSeriesDatasetLowMemory(\"./train_data/train_labels.csv\", \"./train_data/train_data_chunks/\", False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64) # do not shuffle as this would make the data loader very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmericanExpressProfileTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataset_file):\n",
    "        self.dataset = pd.read_pickle(dataset_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.dataset.iloc[idx]\n",
    "        label = row[\"target\"]\n",
    "        data = row.drop(['customer_ID', 'D_63', 'D_64', 'S_2', 'target'], axis=1)\n",
    "\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        data = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Neural Network Architecture #\n",
    "###############################\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(186, 200, 3, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(200, 150, 4, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(150, 100, 2, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AvgPool1d(2, stride=1),\n",
    "        )\n",
    "\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.lin(x)\n",
    "        x = x.view(x.shape[0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Neural Network Model Creation #\n",
    "#################################\n",
    "model = NeuralNetwork()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Neural Network Testing Method #\n",
    "#################################\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = torch.round(pred)\n",
    "            correct += (pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Neural Network Training Method #\n",
    "##################################\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.705754  [    0/321239]\n",
      "loss: 0.394563  [ 6400/321239]\n",
      "loss: 0.273504  [12800/321239]\n",
      "loss: 0.297438  [19200/321239]\n",
      "loss: 0.242142  [25600/321239]\n",
      "loss: 0.453587  [32000/321239]\n",
      "loss: 0.257765  [38400/321239]\n",
      "loss: 0.212439  [44800/321239]\n",
      "loss: 0.287470  [51200/321239]\n",
      "loss: 0.307891  [57600/321239]\n",
      "loss: 0.230182  [64000/321239]\n",
      "loss: 0.340282  [70400/321239]\n",
      "loss: 0.262436  [76800/321239]\n",
      "loss: 0.199133  [83200/321239]\n",
      "loss: 0.174176  [89600/321239]\n",
      "loss: 0.245485  [96000/321239]\n",
      "loss: 0.177652  [102400/321239]\n",
      "loss: 0.239323  [108800/321239]\n",
      "loss: 0.312253  [115200/321239]\n",
      "loss: 0.276236  [121600/321239]\n",
      "loss: 0.392529  [128000/321239]\n",
      "loss: 0.186904  [134400/321239]\n",
      "loss: 0.192381  [140800/321239]\n",
      "loss: 0.314477  [147200/321239]\n",
      "loss: 0.228240  [153600/321239]\n",
      "loss: 0.138480  [160000/321239]\n",
      "loss: 0.206323  [166400/321239]\n",
      "loss: 0.384298  [172800/321239]\n",
      "loss: 0.181248  [179200/321239]\n",
      "loss: 0.230546  [185600/321239]\n",
      "loss: 0.148219  [192000/321239]\n",
      "loss: 0.279463  [198400/321239]\n",
      "loss: 0.219549  [204800/321239]\n",
      "loss: 0.170788  [211200/321239]\n",
      "loss: 0.327338  [217600/321239]\n",
      "loss: 0.193676  [224000/321239]\n",
      "loss: 0.240705  [230400/321239]\n",
      "loss: 0.222260  [236800/321239]\n",
      "loss: 0.289546  [243200/321239]\n",
      "loss: 0.258666  [249600/321239]\n",
      "loss: 0.342157  [256000/321239]\n",
      "loss: 0.253549  [262400/321239]\n",
      "loss: 0.191480  [268800/321239]\n",
      "loss: 0.146954  [275200/321239]\n",
      "loss: 0.181931  [281600/321239]\n",
      "loss: 0.210892  [288000/321239]\n",
      "loss: 0.264636  [294400/321239]\n",
      "loss: 0.269979  [300800/321239]\n",
      "loss: 0.329385  [307200/321239]\n",
      "loss: 0.282938  [313600/321239]\n",
      "loss: 0.205753  [320000/321239]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.250610 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.268384  [    0/321239]\n",
      "loss: 0.297123  [ 6400/321239]\n",
      "loss: 0.254383  [12800/321239]\n",
      "loss: 0.281466  [19200/321239]\n",
      "loss: 0.249222  [25600/321239]\n",
      "loss: 0.442286  [32000/321239]\n",
      "loss: 0.259804  [38400/321239]\n",
      "loss: 0.198558  [44800/321239]\n",
      "loss: 0.270976  [51200/321239]\n",
      "loss: 0.298649  [57600/321239]\n",
      "loss: 0.219913  [64000/321239]\n",
      "loss: 0.336851  [70400/321239]\n",
      "loss: 0.262293  [76800/321239]\n",
      "loss: 0.185980  [83200/321239]\n",
      "loss: 0.155460  [89600/321239]\n",
      "loss: 0.244685  [96000/321239]\n",
      "loss: 0.162607  [102400/321239]\n",
      "loss: 0.220081  [108800/321239]\n",
      "loss: 0.303053  [115200/321239]\n",
      "loss: 0.269549  [121600/321239]\n",
      "loss: 0.383161  [128000/321239]\n",
      "loss: 0.161652  [134400/321239]\n",
      "loss: 0.194211  [140800/321239]\n",
      "loss: 0.318758  [147200/321239]\n",
      "loss: 0.192841  [153600/321239]\n",
      "loss: 0.149309  [160000/321239]\n",
      "loss: 0.200691  [166400/321239]\n",
      "loss: 0.347409  [172800/321239]\n",
      "loss: 0.177493  [179200/321239]\n",
      "loss: 0.228216  [185600/321239]\n",
      "loss: 0.138158  [192000/321239]\n",
      "loss: 0.271571  [198400/321239]\n",
      "loss: 0.222833  [204800/321239]\n",
      "loss: 0.158599  [211200/321239]\n",
      "loss: 0.319012  [217600/321239]\n",
      "loss: 0.188526  [224000/321239]\n",
      "loss: 0.233703  [230400/321239]\n",
      "loss: 0.203849  [236800/321239]\n",
      "loss: 0.258196  [243200/321239]\n",
      "loss: 0.250162  [249600/321239]\n",
      "loss: 0.371579  [256000/321239]\n",
      "loss: 0.234118  [262400/321239]\n",
      "loss: 0.180677  [268800/321239]\n",
      "loss: 0.160745  [275200/321239]\n",
      "loss: 0.174947  [281600/321239]\n",
      "loss: 0.209265  [288000/321239]\n",
      "loss: 0.272171  [294400/321239]\n",
      "loss: 0.281582  [300800/321239]\n",
      "loss: 0.318226  [307200/321239]\n",
      "loss: 0.276873  [313600/321239]\n",
      "loss: 0.190311  [320000/321239]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.247136 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.264847  [    0/321239]\n",
      "loss: 0.281416  [ 6400/321239]\n",
      "loss: 0.231482  [12800/321239]\n",
      "loss: 0.268416  [19200/321239]\n",
      "loss: 0.236647  [25600/321239]\n",
      "loss: 0.444241  [32000/321239]\n",
      "loss: 0.265534  [38400/321239]\n",
      "loss: 0.192847  [44800/321239]\n",
      "loss: 0.264513  [51200/321239]\n",
      "loss: 0.277490  [57600/321239]\n",
      "loss: 0.211726  [64000/321239]\n",
      "loss: 0.334073  [70400/321239]\n",
      "loss: 0.250264  [76800/321239]\n",
      "loss: 0.185250  [83200/321239]\n",
      "loss: 0.145548  [89600/321239]\n",
      "loss: 0.252341  [96000/321239]\n",
      "loss: 0.164021  [102400/321239]\n",
      "loss: 0.219781  [108800/321239]\n",
      "loss: 0.312682  [115200/321239]\n",
      "loss: 0.264498  [121600/321239]\n",
      "loss: 0.364290  [128000/321239]\n",
      "loss: 0.147468  [134400/321239]\n",
      "loss: 0.196906  [140800/321239]\n",
      "loss: 0.330305  [147200/321239]\n",
      "loss: 0.188072  [153600/321239]\n",
      "loss: 0.149480  [160000/321239]\n",
      "loss: 0.193893  [166400/321239]\n",
      "loss: 0.335662  [172800/321239]\n",
      "loss: 0.173685  [179200/321239]\n",
      "loss: 0.233141  [185600/321239]\n",
      "loss: 0.142374  [192000/321239]\n",
      "loss: 0.282101  [198400/321239]\n",
      "loss: 0.219863  [204800/321239]\n",
      "loss: 0.159822  [211200/321239]\n",
      "loss: 0.311816  [217600/321239]\n",
      "loss: 0.173904  [224000/321239]\n",
      "loss: 0.230258  [230400/321239]\n",
      "loss: 0.199594  [236800/321239]\n",
      "loss: 0.237617  [243200/321239]\n",
      "loss: 0.246467  [249600/321239]\n",
      "loss: 0.347625  [256000/321239]\n",
      "loss: 0.226208  [262400/321239]\n",
      "loss: 0.174586  [268800/321239]\n",
      "loss: 0.176179  [275200/321239]\n",
      "loss: 0.163817  [281600/321239]\n",
      "loss: 0.214603  [288000/321239]\n",
      "loss: 0.267861  [294400/321239]\n",
      "loss: 0.287351  [300800/321239]\n",
      "loss: 0.315274  [307200/321239]\n",
      "loss: 0.278602  [313600/321239]\n",
      "loss: 0.172639  [320000/321239]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.246840 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.273629  [    0/321239]\n",
      "loss: 0.276510  [ 6400/321239]\n",
      "loss: 0.215549  [12800/321239]\n",
      "loss: 0.259666  [19200/321239]\n",
      "loss: 0.222907  [25600/321239]\n",
      "loss: 0.437797  [32000/321239]\n",
      "loss: 0.264313  [38400/321239]\n",
      "loss: 0.192230  [44800/321239]\n",
      "loss: 0.259614  [51200/321239]\n",
      "loss: 0.268984  [57600/321239]\n",
      "loss: 0.209397  [64000/321239]\n",
      "loss: 0.333154  [70400/321239]\n",
      "loss: 0.253016  [76800/321239]\n",
      "loss: 0.173991  [83200/321239]\n",
      "loss: 0.142432  [89600/321239]\n",
      "loss: 0.249645  [96000/321239]\n",
      "loss: 0.163423  [102400/321239]\n",
      "loss: 0.213632  [108800/321239]\n",
      "loss: 0.315828  [115200/321239]\n",
      "loss: 0.266772  [121600/321239]\n",
      "loss: 0.357828  [128000/321239]\n",
      "loss: 0.139782  [134400/321239]\n",
      "loss: 0.192933  [140800/321239]\n",
      "loss: 0.314886  [147200/321239]\n",
      "loss: 0.181032  [153600/321239]\n",
      "loss: 0.143153  [160000/321239]\n",
      "loss: 0.193251  [166400/321239]\n",
      "loss: 0.323842  [172800/321239]\n",
      "loss: 0.171118  [179200/321239]\n",
      "loss: 0.222965  [185600/321239]\n",
      "loss: 0.178820  [192000/321239]\n",
      "loss: 0.278281  [198400/321239]\n",
      "loss: 0.212076  [204800/321239]\n",
      "loss: 0.149746  [211200/321239]\n",
      "loss: 0.298800  [217600/321239]\n",
      "loss: 0.172257  [224000/321239]\n",
      "loss: 0.226807  [230400/321239]\n",
      "loss: 0.196307  [236800/321239]\n",
      "loss: 0.233405  [243200/321239]\n",
      "loss: 0.244545  [249600/321239]\n",
      "loss: 0.346589  [256000/321239]\n",
      "loss: 0.224923  [262400/321239]\n",
      "loss: 0.168757  [268800/321239]\n",
      "loss: 0.170194  [275200/321239]\n",
      "loss: 0.154829  [281600/321239]\n",
      "loss: 0.203976  [288000/321239]\n",
      "loss: 0.280237  [294400/321239]\n",
      "loss: 0.258562  [300800/321239]\n",
      "loss: 0.302825  [307200/321239]\n",
      "loss: 0.289785  [313600/321239]\n",
      "loss: 0.182463  [320000/321239]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.246160 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.273136  [    0/321239]\n",
      "loss: 0.276171  [ 6400/321239]\n",
      "loss: 0.222050  [12800/321239]\n",
      "loss: 0.252236  [19200/321239]\n",
      "loss: 0.230099  [25600/321239]\n",
      "loss: 0.439854  [32000/321239]\n",
      "loss: 0.258631  [38400/321239]\n",
      "loss: 0.190587  [44800/321239]\n",
      "loss: 0.258546  [51200/321239]\n",
      "loss: 0.271535  [57600/321239]\n",
      "loss: 0.211732  [64000/321239]\n",
      "loss: 0.322534  [70400/321239]\n",
      "loss: 0.230313  [76800/321239]\n",
      "loss: 0.167869  [83200/321239]\n",
      "loss: 0.138209  [89600/321239]\n",
      "loss: 0.243663  [96000/321239]\n",
      "loss: 0.157948  [102400/321239]\n",
      "loss: 0.211226  [108800/321239]\n",
      "loss: 0.316265  [115200/321239]\n",
      "loss: 0.265281  [121600/321239]\n",
      "loss: 0.351814  [128000/321239]\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Neural Network Training / Testing #\n",
    "#####################################\n",
    "#learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    models[t] = model\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"./model\")\n",
    "#model = torch.load(\"./model\")\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_file(\"./test_data/test_data.csv\",\"./test_data/test_data_chunks/\", customers_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_dataset = AmericanExpressProfileTimeSeriesDatasetLowMemory(\"\", \"./test_data/test_data_chunks/\", True, 1, False)\n",
    "#validation_dataloader = DataLoader(validation_dataset, batch_size=64) # do not shuffle as this would make the data loader very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(dataloader, model, out_file):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data_file = open(f\"{out_file}.csv\", \"w\")\n",
    "        data_file.write(\"customer_ID,prediction\\n\")\n",
    "        for X, customer_ID in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.tolist()\n",
    "            for idx in range(len(customer_ID)):\n",
    "                data_file.write(customer_ID[idx] + \",\" + str(pred[idx])+\"\\n\")\n",
    "        data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_loop(validation_dataloader, model, \"./test_data/test_labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
