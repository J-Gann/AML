{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmericanExpressProfileTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataset_file, transformation=False):\n",
    "        self.dataset = pd.read_csv(dataset_file)\n",
    "        self.transformation = transformation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[[idx]]\n",
    "        label = row[\"target\"].values\n",
    "        data = row.drop(['customer_ID', 'target'], axis=1)\n",
    "        data = data.values[0].tolist()\n",
    "\n",
    "        for idx, value in enumerate(data):\n",
    "            if idx == 0: continue\n",
    "            list_ = value[1:-1].split(\", \")\n",
    "            for idx_l, elem in enumerate(list_):\n",
    "                list_[idx_l] = float(elem)\n",
    "            data[idx] = list_\n",
    "        data = data[1:]\n",
    "\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        data = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        label = label.to(device)\n",
    "        data = data.to(device)\n",
    "        if self.transformation: data = self.transformation(data)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=1, padding=3,  padding_mode='zeros'),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=5, stride=1, padding=2,  padding_mode='zeros'),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1,  padding_mode='zeros'),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.block1 = Block(189, 189)\n",
    "\n",
    "        self.block2 = Block(189, 189)\n",
    "\n",
    "        self.block3 = Block(189, 189)\n",
    "\n",
    "        self.block4 = Block(189, 189)\n",
    "\n",
    "        self.block5 = Block(189, 189)\n",
    "\n",
    "        self.block6 = Block(189, 189)\n",
    "\n",
    "        self.block7 = Block(189, 189)\n",
    "\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(189, 100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "\n",
    "\n",
    "        avg = nn.AvgPool1d(13, stride=1)\n",
    "        x = avg(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(189, 170, kernel_size=8, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(170, 140, kernel_size=5, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(140, 100, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(170, 189, kernel_size=8, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(140, 170, kernel_size=5, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(100, 140, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, convolution_layers, linear_layers, activation, final_activation):\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(197, 256, 2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 512, 2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=1),\n",
    "            nn.Conv1d(512, 1024, 2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(1024, 1024, 2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=1),\n",
    "            nn.Conv1d(1024, 1024, 2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(1024, 1024, 2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "#model = ConvolutionalNetwork()\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        hidden = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)     \n",
    "        out = self.linear(hidden)\n",
    "        return out\n",
    "\n",
    "model = LSTM(189, 10, 10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = torch.round(pred)\n",
    "            correct += (pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "lr_lambda = 0.5 # try 0.7\n",
    "batch_size = 16\n",
    "shuffle=True\n",
    "train_test_ration = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = AmericanExpressProfileTimeSeriesDataset(\"transformed_dataset_normalized.csv\")#, transformation=lambda data: data.T)\n",
    "\n",
    "train_size = int(train_test_ration * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.699423  [    0/367130]\n",
      "loss: 0.707796  [ 1600/367130]\n",
      "loss: 0.181046  [ 3200/367130]\n",
      "loss: 0.212703  [ 4800/367130]\n",
      "loss: 0.232851  [ 6400/367130]\n",
      "loss: 0.180716  [ 8000/367130]\n",
      "loss: 0.195914  [ 9600/367130]\n",
      "loss: 0.271660  [11200/367130]\n",
      "loss: 0.188766  [12800/367130]\n",
      "loss: 0.101777  [14400/367130]\n",
      "loss: 0.181551  [16000/367130]\n",
      "loss: 0.136989  [17600/367130]\n",
      "loss: 0.235838  [19200/367130]\n",
      "loss: 0.147612  [20800/367130]\n",
      "loss: 0.344412  [22400/367130]\n",
      "loss: 0.221237  [24000/367130]\n",
      "loss: 0.227699  [25600/367130]\n",
      "loss: 0.927617  [27200/367130]\n",
      "loss: 0.339461  [28800/367130]\n",
      "loss: 0.465045  [30400/367130]\n",
      "loss: 0.335338  [32000/367130]\n",
      "loss: 0.698721  [33600/367130]\n",
      "loss: 0.451322  [35200/367130]\n",
      "loss: 0.090654  [36800/367130]\n",
      "loss: 0.193411  [38400/367130]\n",
      "loss: 0.290839  [40000/367130]\n",
      "loss: 0.384459  [41600/367130]\n",
      "loss: 0.162070  [43200/367130]\n",
      "loss: 0.175725  [44800/367130]\n",
      "loss: 0.268424  [46400/367130]\n",
      "loss: 0.185608  [48000/367130]\n",
      "loss: 0.361771  [49600/367130]\n",
      "loss: 0.312293  [51200/367130]\n",
      "loss: 0.342829  [52800/367130]\n",
      "loss: 0.081657  [54400/367130]\n",
      "loss: 0.399613  [56000/367130]\n",
      "loss: 0.398341  [57600/367130]\n",
      "loss: 0.368301  [59200/367130]\n",
      "loss: 0.364228  [60800/367130]\n",
      "loss: 0.211717  [62400/367130]\n",
      "loss: 0.495919  [64000/367130]\n",
      "loss: 0.400802  [65600/367130]\n",
      "loss: 0.082730  [67200/367130]\n",
      "loss: 0.376349  [68800/367130]\n",
      "loss: 0.308494  [70400/367130]\n",
      "loss: 0.245808  [72000/367130]\n",
      "loss: 0.279681  [73600/367130]\n",
      "loss: 0.192180  [75200/367130]\n",
      "loss: 0.195358  [76800/367130]\n",
      "loss: 0.198291  [78400/367130]\n",
      "loss: 0.243300  [80000/367130]\n",
      "loss: 0.107965  [81600/367130]\n",
      "loss: 0.226691  [83200/367130]\n",
      "loss: 0.179794  [84800/367130]\n",
      "loss: 0.135699  [86400/367130]\n",
      "loss: 0.405664  [88000/367130]\n",
      "loss: 0.225712  [89600/367130]\n",
      "loss: 0.101967  [91200/367130]\n",
      "loss: 0.135401  [92800/367130]\n",
      "loss: 0.070492  [94400/367130]\n",
      "loss: 0.440167  [96000/367130]\n",
      "loss: 0.163111  [97600/367130]\n",
      "loss: 0.108979  [99200/367130]\n",
      "loss: 0.204504  [100800/367130]\n",
      "loss: 0.136904  [102400/367130]\n",
      "loss: 0.345955  [104000/367130]\n",
      "loss: 0.260758  [105600/367130]\n",
      "loss: 0.243562  [107200/367130]\n",
      "loss: 0.176436  [108800/367130]\n",
      "loss: 0.201119  [110400/367130]\n",
      "loss: 0.284098  [112000/367130]\n",
      "loss: 0.489774  [113600/367130]\n",
      "loss: 0.044892  [115200/367130]\n",
      "loss: 0.573253  [116800/367130]\n",
      "loss: 0.222029  [118400/367130]\n",
      "loss: 0.406162  [120000/367130]\n",
      "loss: 0.464192  [121600/367130]\n",
      "loss: 0.166715  [123200/367130]\n",
      "loss: 0.180205  [124800/367130]\n",
      "loss: 0.313486  [126400/367130]\n",
      "loss: 0.462258  [128000/367130]\n",
      "loss: 0.207841  [129600/367130]\n",
      "loss: 0.095893  [131200/367130]\n",
      "loss: 0.412470  [132800/367130]\n",
      "loss: 0.216854  [134400/367130]\n",
      "loss: 0.273074  [136000/367130]\n",
      "loss: 0.187315  [137600/367130]\n",
      "loss: 0.188685  [139200/367130]\n",
      "loss: 0.197491  [140800/367130]\n",
      "loss: 0.199408  [142400/367130]\n",
      "loss: 0.214466  [144000/367130]\n",
      "loss: 0.359616  [145600/367130]\n",
      "loss: 0.278611  [147200/367130]\n",
      "loss: 0.097972  [148800/367130]\n",
      "loss: 0.220507  [150400/367130]\n",
      "loss: 0.111649  [152000/367130]\n",
      "loss: 0.238769  [153600/367130]\n",
      "loss: 0.110271  [155200/367130]\n",
      "loss: 0.182145  [156800/367130]\n",
      "loss: 0.264130  [158400/367130]\n",
      "loss: 0.308536  [160000/367130]\n",
      "loss: 0.185468  [161600/367130]\n",
      "loss: 0.229116  [163200/367130]\n",
      "loss: 0.081158  [164800/367130]\n",
      "loss: 0.153017  [166400/367130]\n",
      "loss: 0.275905  [168000/367130]\n",
      "loss: 0.099756  [169600/367130]\n",
      "loss: 0.165750  [171200/367130]\n",
      "loss: 0.060899  [172800/367130]\n",
      "loss: 0.306027  [174400/367130]\n",
      "loss: 0.106613  [176000/367130]\n",
      "loss: 0.391649  [177600/367130]\n",
      "loss: 0.150284  [179200/367130]\n",
      "loss: 0.376820  [180800/367130]\n",
      "loss: 0.285134  [182400/367130]\n",
      "loss: 0.227159  [184000/367130]\n",
      "loss: 0.216212  [185600/367130]\n",
      "loss: 0.193312  [187200/367130]\n",
      "loss: 0.256494  [188800/367130]\n",
      "loss: 0.297853  [190400/367130]\n",
      "loss: 0.475724  [192000/367130]\n",
      "loss: 0.318705  [193600/367130]\n",
      "loss: 0.648416  [195200/367130]\n",
      "loss: 0.168529  [196800/367130]\n",
      "loss: 0.061228  [198400/367130]\n",
      "loss: 0.178652  [200000/367130]\n",
      "loss: 0.387071  [201600/367130]\n",
      "loss: 0.349948  [203200/367130]\n",
      "loss: 0.143641  [204800/367130]\n",
      "loss: 0.292747  [206400/367130]\n",
      "loss: 0.095392  [208000/367130]\n",
      "loss: 0.071858  [209600/367130]\n",
      "loss: 0.241444  [211200/367130]\n",
      "loss: 0.120154  [212800/367130]\n",
      "loss: 0.308953  [214400/367130]\n",
      "loss: 0.266505  [216000/367130]\n",
      "loss: 0.286099  [217600/367130]\n",
      "loss: 0.132516  [219200/367130]\n",
      "loss: 0.088865  [220800/367130]\n",
      "loss: 0.144936  [222400/367130]\n",
      "loss: 0.131519  [224000/367130]\n",
      "loss: 0.145733  [225600/367130]\n",
      "loss: 0.385931  [227200/367130]\n",
      "loss: 0.109973  [228800/367130]\n",
      "loss: 0.093947  [230400/367130]\n",
      "loss: 0.377204  [232000/367130]\n",
      "loss: 0.198296  [233600/367130]\n",
      "loss: 0.237161  [235200/367130]\n",
      "loss: 0.098103  [236800/367130]\n",
      "loss: 0.296093  [238400/367130]\n",
      "loss: 0.089064  [240000/367130]\n",
      "loss: 0.099150  [241600/367130]\n",
      "loss: 0.200509  [243200/367130]\n",
      "loss: 0.448505  [244800/367130]\n",
      "loss: 0.293107  [246400/367130]\n",
      "loss: 0.128424  [248000/367130]\n",
      "loss: 0.029758  [249600/367130]\n",
      "loss: 0.414496  [251200/367130]\n",
      "loss: 0.321910  [252800/367130]\n",
      "loss: 0.219383  [254400/367130]\n",
      "loss: 0.249619  [256000/367130]\n",
      "loss: 0.190581  [257600/367130]\n",
      "loss: 0.249939  [259200/367130]\n",
      "loss: 0.291843  [260800/367130]\n",
      "loss: 0.390910  [262400/367130]\n",
      "loss: 0.417054  [264000/367130]\n",
      "loss: 0.245382  [265600/367130]\n",
      "loss: 0.083811  [267200/367130]\n",
      "loss: 0.204031  [268800/367130]\n",
      "loss: 0.118914  [270400/367130]\n",
      "loss: 0.175545  [272000/367130]\n",
      "loss: 0.337216  [273600/367130]\n",
      "loss: 0.370060  [275200/367130]\n",
      "loss: 0.144144  [276800/367130]\n",
      "loss: 0.275546  [278400/367130]\n",
      "loss: 0.184696  [280000/367130]\n",
      "loss: 0.130549  [281600/367130]\n",
      "loss: 0.148256  [283200/367130]\n",
      "loss: 0.032334  [284800/367130]\n",
      "loss: 0.149692  [286400/367130]\n",
      "loss: 0.096262  [288000/367130]\n",
      "loss: 0.052233  [289600/367130]\n",
      "loss: 0.167641  [291200/367130]\n",
      "loss: 0.122521  [292800/367130]\n",
      "loss: 0.702647  [294400/367130]\n",
      "loss: 0.162502  [296000/367130]\n",
      "loss: 0.194865  [297600/367130]\n",
      "loss: 0.253422  [299200/367130]\n",
      "loss: 0.164650  [300800/367130]\n",
      "loss: 0.211250  [302400/367130]\n",
      "loss: 0.181140  [304000/367130]\n",
      "loss: 0.123441  [305600/367130]\n",
      "loss: 0.447661  [307200/367130]\n",
      "loss: 0.132106  [308800/367130]\n",
      "loss: 0.169538  [310400/367130]\n",
      "loss: 0.160498  [312000/367130]\n",
      "loss: 0.155398  [313600/367130]\n",
      "loss: 0.286856  [315200/367130]\n",
      "loss: 0.097490  [316800/367130]\n",
      "loss: 0.204347  [318400/367130]\n",
      "loss: 0.136912  [320000/367130]\n",
      "loss: 0.086516  [321600/367130]\n",
      "loss: 0.193099  [323200/367130]\n",
      "loss: 0.126508  [324800/367130]\n",
      "loss: 0.091157  [326400/367130]\n",
      "loss: 0.348804  [328000/367130]\n",
      "loss: 0.272518  [329600/367130]\n",
      "loss: 0.110232  [331200/367130]\n",
      "loss: 0.257754  [332800/367130]\n",
      "loss: 0.287730  [334400/367130]\n",
      "loss: 0.174843  [336000/367130]\n",
      "loss: 0.162129  [337600/367130]\n",
      "loss: 0.113516  [339200/367130]\n",
      "loss: 0.079822  [340800/367130]\n",
      "loss: 0.149331  [342400/367130]\n",
      "loss: 0.332405  [344000/367130]\n",
      "loss: 0.246676  [345600/367130]\n",
      "loss: 0.192341  [347200/367130]\n",
      "loss: 0.333146  [348800/367130]\n",
      "loss: 0.157629  [350400/367130]\n",
      "loss: 0.099159  [352000/367130]\n",
      "loss: 0.230948  [353600/367130]\n",
      "loss: 0.834967  [355200/367130]\n",
      "loss: 0.331320  [356800/367130]\n",
      "loss: 0.126941  [358400/367130]\n",
      "loss: 0.148577  [360000/367130]\n",
      "loss: 0.256061  [361600/367130]\n",
      "loss: 0.169652  [363200/367130]\n",
      "loss: 0.161513  [364800/367130]\n",
      "loss: 0.625958  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.233875 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.238634  [    0/367130]\n",
      "loss: 0.492307  [ 1600/367130]\n",
      "loss: 0.183626  [ 3200/367130]\n",
      "loss: 0.236714  [ 4800/367130]\n",
      "loss: 0.128323  [ 6400/367130]\n",
      "loss: 0.413550  [ 8000/367130]\n",
      "loss: 0.224842  [ 9600/367130]\n",
      "loss: 0.093860  [11200/367130]\n",
      "loss: 0.114076  [12800/367130]\n",
      "loss: 0.168818  [14400/367130]\n",
      "loss: 0.173331  [16000/367130]\n",
      "loss: 0.232832  [17600/367130]\n",
      "loss: 0.474814  [19200/367130]\n",
      "loss: 0.116097  [20800/367130]\n",
      "loss: 0.198782  [22400/367130]\n",
      "loss: 0.467207  [24000/367130]\n",
      "loss: 0.287369  [25600/367130]\n",
      "loss: 0.329950  [27200/367130]\n",
      "loss: 0.108192  [28800/367130]\n",
      "loss: 0.185900  [30400/367130]\n",
      "loss: 0.206590  [32000/367130]\n",
      "loss: 0.122293  [33600/367130]\n",
      "loss: 0.146670  [35200/367130]\n",
      "loss: 0.245609  [36800/367130]\n",
      "loss: 0.286465  [38400/367130]\n",
      "loss: 0.182281  [40000/367130]\n",
      "loss: 0.043121  [41600/367130]\n",
      "loss: 0.192755  [43200/367130]\n",
      "loss: 0.095369  [44800/367130]\n",
      "loss: 0.218166  [46400/367130]\n",
      "loss: 0.204629  [48000/367130]\n",
      "loss: 0.131814  [49600/367130]\n",
      "loss: 0.240757  [51200/367130]\n",
      "loss: 0.218414  [52800/367130]\n",
      "loss: 0.146085  [54400/367130]\n",
      "loss: 0.163558  [56000/367130]\n",
      "loss: 0.215071  [57600/367130]\n",
      "loss: 0.206751  [59200/367130]\n",
      "loss: 0.407122  [60800/367130]\n",
      "loss: 0.240180  [62400/367130]\n",
      "loss: 0.261575  [64000/367130]\n",
      "loss: 0.303658  [65600/367130]\n",
      "loss: 0.294025  [67200/367130]\n",
      "loss: 0.177489  [68800/367130]\n",
      "loss: 0.103508  [70400/367130]\n",
      "loss: 0.369313  [72000/367130]\n",
      "loss: 0.161244  [73600/367130]\n",
      "loss: 0.105321  [75200/367130]\n",
      "loss: 0.313350  [76800/367130]\n",
      "loss: 0.194175  [78400/367130]\n",
      "loss: 0.310257  [80000/367130]\n",
      "loss: 0.164617  [81600/367130]\n",
      "loss: 0.186166  [83200/367130]\n",
      "loss: 0.308279  [84800/367130]\n",
      "loss: 0.126773  [86400/367130]\n",
      "loss: 0.087534  [88000/367130]\n",
      "loss: 0.391159  [89600/367130]\n",
      "loss: 0.170245  [91200/367130]\n",
      "loss: 0.370568  [92800/367130]\n",
      "loss: 0.089971  [94400/367130]\n",
      "loss: 0.198620  [96000/367130]\n",
      "loss: 0.139214  [97600/367130]\n",
      "loss: 0.168661  [99200/367130]\n",
      "loss: 0.173794  [100800/367130]\n",
      "loss: 0.364264  [102400/367130]\n",
      "loss: 0.173436  [104000/367130]\n",
      "loss: 0.156271  [105600/367130]\n",
      "loss: 0.099157  [107200/367130]\n",
      "loss: 0.150100  [108800/367130]\n",
      "loss: 0.086833  [110400/367130]\n",
      "loss: 0.420930  [112000/367130]\n",
      "loss: 0.254656  [113600/367130]\n",
      "loss: 0.125451  [115200/367130]\n",
      "loss: 0.138943  [116800/367130]\n",
      "loss: 0.344805  [118400/367130]\n",
      "loss: 0.075071  [120000/367130]\n",
      "loss: 0.299687  [121600/367130]\n",
      "loss: 0.290565  [123200/367130]\n",
      "loss: 0.064235  [124800/367130]\n",
      "loss: 0.349863  [126400/367130]\n",
      "loss: 0.080723  [128000/367130]\n",
      "loss: 0.218207  [129600/367130]\n",
      "loss: 0.244886  [131200/367130]\n",
      "loss: 0.188988  [132800/367130]\n",
      "loss: 0.188957  [134400/367130]\n",
      "loss: 0.236036  [136000/367130]\n",
      "loss: 0.126906  [137600/367130]\n",
      "loss: 0.510486  [139200/367130]\n",
      "loss: 0.291407  [140800/367130]\n",
      "loss: 0.142183  [142400/367130]\n",
      "loss: 0.059904  [144000/367130]\n",
      "loss: 0.185831  [145600/367130]\n",
      "loss: 0.329492  [147200/367130]\n",
      "loss: 0.141739  [148800/367130]\n",
      "loss: 0.256030  [150400/367130]\n",
      "loss: 0.365817  [152000/367130]\n",
      "loss: 0.129924  [153600/367130]\n",
      "loss: 0.094192  [155200/367130]\n",
      "loss: 0.464107  [156800/367130]\n",
      "loss: 0.138848  [158400/367130]\n",
      "loss: 0.364345  [160000/367130]\n",
      "loss: 0.154980  [161600/367130]\n",
      "loss: 0.193858  [163200/367130]\n",
      "loss: 0.143374  [164800/367130]\n",
      "loss: 0.305278  [166400/367130]\n",
      "loss: 0.524817  [168000/367130]\n",
      "loss: 0.248842  [169600/367130]\n",
      "loss: 0.308578  [171200/367130]\n",
      "loss: 0.128097  [172800/367130]\n",
      "loss: 0.149934  [174400/367130]\n",
      "loss: 0.294909  [176000/367130]\n",
      "loss: 0.224263  [177600/367130]\n",
      "loss: 0.363939  [179200/367130]\n",
      "loss: 0.352411  [180800/367130]\n",
      "loss: 0.248277  [182400/367130]\n",
      "loss: 0.031336  [184000/367130]\n",
      "loss: 0.228400  [185600/367130]\n",
      "loss: 0.345526  [187200/367130]\n",
      "loss: 0.259181  [188800/367130]\n",
      "loss: 0.074148  [190400/367130]\n",
      "loss: 0.191722  [192000/367130]\n",
      "loss: 0.142986  [193600/367130]\n",
      "loss: 0.173256  [195200/367130]\n",
      "loss: 0.659088  [196800/367130]\n",
      "loss: 0.072586  [198400/367130]\n",
      "loss: 0.272015  [200000/367130]\n",
      "loss: 0.175661  [201600/367130]\n",
      "loss: 0.549410  [203200/367130]\n",
      "loss: 0.088585  [204800/367130]\n",
      "loss: 0.105117  [206400/367130]\n",
      "loss: 0.318627  [208000/367130]\n",
      "loss: 0.143721  [209600/367130]\n",
      "loss: 0.208515  [211200/367130]\n",
      "loss: 0.237187  [212800/367130]\n",
      "loss: 0.308318  [214400/367130]\n",
      "loss: 0.292622  [216000/367130]\n",
      "loss: 0.070639  [217600/367130]\n",
      "loss: 0.246186  [219200/367130]\n",
      "loss: 0.149408  [220800/367130]\n",
      "loss: 0.280346  [222400/367130]\n",
      "loss: 0.247600  [224000/367130]\n",
      "loss: 0.210046  [225600/367130]\n",
      "loss: 0.087803  [227200/367130]\n",
      "loss: 0.415306  [228800/367130]\n",
      "loss: 0.418195  [230400/367130]\n",
      "loss: 0.316757  [232000/367130]\n",
      "loss: 0.072403  [233600/367130]\n",
      "loss: 0.289125  [235200/367130]\n",
      "loss: 0.179393  [236800/367130]\n",
      "loss: 0.354436  [238400/367130]\n",
      "loss: 0.406609  [240000/367130]\n",
      "loss: 0.168155  [241600/367130]\n",
      "loss: 0.541885  [243200/367130]\n",
      "loss: 0.397922  [244800/367130]\n",
      "loss: 0.386272  [246400/367130]\n",
      "loss: 0.416988  [248000/367130]\n",
      "loss: 0.079118  [249600/367130]\n",
      "loss: 0.105784  [251200/367130]\n",
      "loss: 0.321992  [252800/367130]\n",
      "loss: 0.225842  [254400/367130]\n",
      "loss: 0.082533  [256000/367130]\n",
      "loss: 0.159983  [257600/367130]\n",
      "loss: 0.189540  [259200/367130]\n",
      "loss: 0.122643  [260800/367130]\n",
      "loss: 0.200966  [262400/367130]\n",
      "loss: 0.363389  [264000/367130]\n",
      "loss: 0.224896  [265600/367130]\n",
      "loss: 0.086372  [267200/367130]\n",
      "loss: 0.277913  [268800/367130]\n",
      "loss: 0.179499  [270400/367130]\n",
      "loss: 0.349428  [272000/367130]\n",
      "loss: 0.174147  [273600/367130]\n",
      "loss: 0.128964  [275200/367130]\n",
      "loss: 0.152848  [276800/367130]\n",
      "loss: 0.135656  [278400/367130]\n",
      "loss: 0.193799  [280000/367130]\n",
      "loss: 0.235796  [281600/367130]\n",
      "loss: 0.119446  [283200/367130]\n",
      "loss: 0.234997  [284800/367130]\n",
      "loss: 0.110459  [286400/367130]\n",
      "loss: 0.177493  [288000/367130]\n",
      "loss: 0.245751  [289600/367130]\n",
      "loss: 0.464461  [291200/367130]\n",
      "loss: 0.205057  [292800/367130]\n",
      "loss: 0.186900  [294400/367130]\n",
      "loss: 0.042502  [296000/367130]\n",
      "loss: 0.328073  [297600/367130]\n",
      "loss: 0.399282  [299200/367130]\n",
      "loss: 0.219438  [300800/367130]\n",
      "loss: 0.145863  [302400/367130]\n",
      "loss: 0.073838  [304000/367130]\n",
      "loss: 0.034441  [305600/367130]\n",
      "loss: 0.198386  [307200/367130]\n",
      "loss: 0.088065  [308800/367130]\n",
      "loss: 0.134095  [310400/367130]\n",
      "loss: 0.199056  [312000/367130]\n",
      "loss: 0.292171  [313600/367130]\n",
      "loss: 0.126819  [315200/367130]\n",
      "loss: 0.277825  [316800/367130]\n",
      "loss: 0.139848  [318400/367130]\n",
      "loss: 0.783749  [320000/367130]\n",
      "loss: 0.058923  [321600/367130]\n",
      "loss: 0.121852  [323200/367130]\n",
      "loss: 0.146387  [324800/367130]\n",
      "loss: 0.393035  [326400/367130]\n",
      "loss: 0.108819  [328000/367130]\n",
      "loss: 0.269611  [329600/367130]\n",
      "loss: 0.142116  [331200/367130]\n",
      "loss: 0.030009  [332800/367130]\n",
      "loss: 0.315346  [334400/367130]\n",
      "loss: 0.272028  [336000/367130]\n",
      "loss: 0.504137  [337600/367130]\n",
      "loss: 0.138692  [339200/367130]\n",
      "loss: 0.356080  [340800/367130]\n",
      "loss: 0.272994  [342400/367130]\n",
      "loss: 0.132244  [344000/367130]\n",
      "loss: 0.219128  [345600/367130]\n",
      "loss: 0.102405  [347200/367130]\n",
      "loss: 0.186143  [348800/367130]\n",
      "loss: 0.110378  [350400/367130]\n",
      "loss: 0.161881  [352000/367130]\n",
      "loss: 0.230599  [353600/367130]\n",
      "loss: 0.227726  [355200/367130]\n",
      "loss: 0.254827  [356800/367130]\n",
      "loss: 0.450191  [358400/367130]\n",
      "loss: 0.376033  [360000/367130]\n",
      "loss: 0.202026  [361600/367130]\n",
      "loss: 0.448690  [363200/367130]\n",
      "loss: 0.276393  [364800/367130]\n",
      "loss: 0.172070  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.228392 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.498758  [    0/367130]\n",
      "loss: 0.212669  [ 1600/367130]\n",
      "loss: 0.095758  [ 3200/367130]\n",
      "loss: 0.067136  [ 4800/367130]\n",
      "loss: 0.239022  [ 6400/367130]\n",
      "loss: 0.180038  [ 8000/367130]\n",
      "loss: 0.586249  [ 9600/367130]\n",
      "loss: 0.328471  [11200/367130]\n",
      "loss: 0.181905  [12800/367130]\n",
      "loss: 0.069102  [14400/367130]\n",
      "loss: 0.102074  [16000/367130]\n",
      "loss: 0.310127  [17600/367130]\n",
      "loss: 0.180926  [19200/367130]\n",
      "loss: 0.143416  [20800/367130]\n",
      "loss: 0.358784  [22400/367130]\n",
      "loss: 0.238421  [24000/367130]\n",
      "loss: 0.147348  [25600/367130]\n",
      "loss: 0.318767  [27200/367130]\n",
      "loss: 0.179249  [28800/367130]\n",
      "loss: 0.119758  [30400/367130]\n",
      "loss: 0.214252  [32000/367130]\n",
      "loss: 0.179634  [33600/367130]\n",
      "loss: 0.155562  [35200/367130]\n",
      "loss: 0.115461  [36800/367130]\n",
      "loss: 0.088138  [38400/367130]\n",
      "loss: 0.407475  [40000/367130]\n",
      "loss: 0.066540  [41600/367130]\n",
      "loss: 0.584043  [43200/367130]\n",
      "loss: 0.329499  [44800/367130]\n",
      "loss: 0.287446  [46400/367130]\n",
      "loss: 0.043157  [48000/367130]\n",
      "loss: 0.212312  [49600/367130]\n",
      "loss: 0.246081  [51200/367130]\n",
      "loss: 0.184841  [52800/367130]\n",
      "loss: 0.364021  [54400/367130]\n",
      "loss: 0.343130  [56000/367130]\n",
      "loss: 0.416382  [57600/367130]\n",
      "loss: 0.110340  [59200/367130]\n",
      "loss: 0.044195  [60800/367130]\n",
      "loss: 0.122662  [62400/367130]\n",
      "loss: 0.413386  [64000/367130]\n",
      "loss: 0.259681  [65600/367130]\n",
      "loss: 0.171729  [67200/367130]\n",
      "loss: 0.310521  [68800/367130]\n",
      "loss: 0.194990  [70400/367130]\n",
      "loss: 0.101256  [72000/367130]\n",
      "loss: 0.138706  [73600/367130]\n",
      "loss: 0.160044  [75200/367130]\n",
      "loss: 0.120848  [76800/367130]\n",
      "loss: 0.183783  [78400/367130]\n",
      "loss: 0.245282  [80000/367130]\n",
      "loss: 0.215437  [81600/367130]\n",
      "loss: 0.270059  [83200/367130]\n",
      "loss: 0.244838  [84800/367130]\n",
      "loss: 0.186146  [86400/367130]\n",
      "loss: 0.238137  [88000/367130]\n",
      "loss: 0.191053  [89600/367130]\n",
      "loss: 0.182209  [91200/367130]\n",
      "loss: 0.316311  [92800/367130]\n",
      "loss: 0.214838  [94400/367130]\n",
      "loss: 0.210772  [96000/367130]\n",
      "loss: 0.272333  [97600/367130]\n",
      "loss: 0.165649  [99200/367130]\n",
      "loss: 0.086128  [100800/367130]\n",
      "loss: 0.138229  [102400/367130]\n",
      "loss: 0.362708  [104000/367130]\n",
      "loss: 0.126081  [105600/367130]\n",
      "loss: 0.138357  [107200/367130]\n",
      "loss: 0.258641  [108800/367130]\n",
      "loss: 0.169757  [110400/367130]\n",
      "loss: 0.255795  [112000/367130]\n",
      "loss: 0.074955  [113600/367130]\n",
      "loss: 0.406353  [115200/367130]\n",
      "loss: 0.177896  [116800/367130]\n",
      "loss: 0.188110  [118400/367130]\n",
      "loss: 0.436255  [120000/367130]\n",
      "loss: 0.210715  [121600/367130]\n",
      "loss: 0.426619  [123200/367130]\n",
      "loss: 0.227933  [124800/367130]\n",
      "loss: 0.366716  [126400/367130]\n",
      "loss: 0.413466  [128000/367130]\n",
      "loss: 0.179576  [129600/367130]\n",
      "loss: 0.328878  [131200/367130]\n",
      "loss: 0.190397  [132800/367130]\n",
      "loss: 0.227479  [134400/367130]\n",
      "loss: 0.230677  [136000/367130]\n",
      "loss: 0.244768  [137600/367130]\n",
      "loss: 0.138323  [139200/367130]\n",
      "loss: 0.089567  [140800/367130]\n",
      "loss: 0.216933  [142400/367130]\n",
      "loss: 0.146615  [144000/367130]\n",
      "loss: 0.469107  [145600/367130]\n",
      "loss: 0.101786  [147200/367130]\n",
      "loss: 0.243154  [148800/367130]\n",
      "loss: 0.185312  [150400/367130]\n",
      "loss: 0.320088  [152000/367130]\n",
      "loss: 0.123212  [153600/367130]\n",
      "loss: 0.139920  [155200/367130]\n",
      "loss: 0.098408  [156800/367130]\n",
      "loss: 0.149849  [158400/367130]\n",
      "loss: 0.177568  [160000/367130]\n",
      "loss: 0.072559  [161600/367130]\n",
      "loss: 0.117686  [163200/367130]\n",
      "loss: 0.161826  [164800/367130]\n",
      "loss: 0.236943  [166400/367130]\n",
      "loss: 0.245194  [168000/367130]\n",
      "loss: 0.214856  [169600/367130]\n",
      "loss: 0.247985  [171200/367130]\n",
      "loss: 0.187429  [172800/367130]\n",
      "loss: 0.098261  [174400/367130]\n",
      "loss: 0.144885  [176000/367130]\n",
      "loss: 0.077400  [177600/367130]\n",
      "loss: 0.260748  [179200/367130]\n",
      "loss: 0.106559  [180800/367130]\n",
      "loss: 0.338951  [182400/367130]\n",
      "loss: 0.151631  [184000/367130]\n",
      "loss: 0.236833  [185600/367130]\n",
      "loss: 0.295342  [187200/367130]\n",
      "loss: 0.134164  [188800/367130]\n",
      "loss: 0.305916  [190400/367130]\n",
      "loss: 0.102026  [192000/367130]\n",
      "loss: 0.426550  [193600/367130]\n",
      "loss: 0.150488  [195200/367130]\n",
      "loss: 0.084709  [196800/367130]\n",
      "loss: 0.259293  [198400/367130]\n",
      "loss: 0.187458  [200000/367130]\n",
      "loss: 0.075565  [201600/367130]\n",
      "loss: 0.186955  [203200/367130]\n",
      "loss: 0.150018  [204800/367130]\n",
      "loss: 0.125382  [206400/367130]\n",
      "loss: 0.150783  [208000/367130]\n",
      "loss: 0.140977  [209600/367130]\n",
      "loss: 0.141592  [211200/367130]\n",
      "loss: 0.029838  [212800/367130]\n",
      "loss: 0.225782  [214400/367130]\n",
      "loss: 0.130569  [216000/367130]\n",
      "loss: 0.236359  [217600/367130]\n",
      "loss: 0.242083  [219200/367130]\n",
      "loss: 0.159164  [220800/367130]\n",
      "loss: 0.286813  [222400/367130]\n",
      "loss: 0.094583  [224000/367130]\n",
      "loss: 0.523101  [225600/367130]\n",
      "loss: 0.302546  [227200/367130]\n",
      "loss: 0.177821  [228800/367130]\n",
      "loss: 0.259204  [230400/367130]\n",
      "loss: 0.237796  [232000/367130]\n",
      "loss: 0.164894  [233600/367130]\n",
      "loss: 0.551482  [235200/367130]\n",
      "loss: 0.013386  [236800/367130]\n",
      "loss: 0.264306  [238400/367130]\n",
      "loss: 0.139974  [240000/367130]\n",
      "loss: 0.347279  [241600/367130]\n",
      "loss: 0.561328  [243200/367130]\n",
      "loss: 0.102921  [244800/367130]\n",
      "loss: 0.241900  [246400/367130]\n",
      "loss: 0.114534  [248000/367130]\n",
      "loss: 0.191273  [249600/367130]\n",
      "loss: 0.539277  [251200/367130]\n",
      "loss: 0.134954  [252800/367130]\n",
      "loss: 0.307806  [254400/367130]\n",
      "loss: 0.033493  [256000/367130]\n",
      "loss: 0.113106  [257600/367130]\n",
      "loss: 0.274853  [259200/367130]\n",
      "loss: 0.330988  [260800/367130]\n",
      "loss: 0.140629  [262400/367130]\n",
      "loss: 0.253887  [264000/367130]\n",
      "loss: 0.175841  [265600/367130]\n",
      "loss: 0.288332  [267200/367130]\n",
      "loss: 0.220934  [268800/367130]\n",
      "loss: 0.034922  [270400/367130]\n",
      "loss: 0.291843  [272000/367130]\n",
      "loss: 0.149721  [273600/367130]\n",
      "loss: 0.107403  [275200/367130]\n",
      "loss: 0.151629  [276800/367130]\n",
      "loss: 0.490750  [278400/367130]\n",
      "loss: 0.305258  [280000/367130]\n",
      "loss: 0.211587  [281600/367130]\n",
      "loss: 0.280291  [283200/367130]\n",
      "loss: 0.088822  [284800/367130]\n",
      "loss: 0.443944  [286400/367130]\n",
      "loss: 0.194981  [288000/367130]\n",
      "loss: 0.578164  [289600/367130]\n",
      "loss: 0.515103  [291200/367130]\n",
      "loss: 0.081750  [292800/367130]\n",
      "loss: 0.154267  [294400/367130]\n",
      "loss: 0.238533  [296000/367130]\n",
      "loss: 0.328796  [297600/367130]\n",
      "loss: 0.041566  [299200/367130]\n",
      "loss: 0.274074  [300800/367130]\n",
      "loss: 0.165904  [302400/367130]\n",
      "loss: 0.397904  [304000/367130]\n",
      "loss: 0.203625  [305600/367130]\n",
      "loss: 0.188312  [307200/367130]\n",
      "loss: 0.835076  [308800/367130]\n",
      "loss: 0.108663  [310400/367130]\n",
      "loss: 0.242069  [312000/367130]\n",
      "loss: 0.219913  [313600/367130]\n",
      "loss: 0.181223  [315200/367130]\n",
      "loss: 0.162557  [316800/367130]\n",
      "loss: 0.138427  [318400/367130]\n",
      "loss: 0.274723  [320000/367130]\n",
      "loss: 0.053399  [321600/367130]\n",
      "loss: 0.092471  [323200/367130]\n",
      "loss: 0.130631  [324800/367130]\n",
      "loss: 0.488152  [326400/367130]\n",
      "loss: 0.078699  [328000/367130]\n",
      "loss: 0.129393  [329600/367130]\n",
      "loss: 0.163373  [331200/367130]\n",
      "loss: 0.453907  [332800/367130]\n",
      "loss: 0.360404  [334400/367130]\n",
      "loss: 0.231860  [336000/367130]\n",
      "loss: 0.079504  [337600/367130]\n",
      "loss: 0.211459  [339200/367130]\n",
      "loss: 0.250265  [340800/367130]\n",
      "loss: 0.155176  [342400/367130]\n",
      "loss: 0.330187  [344000/367130]\n",
      "loss: 0.200772  [345600/367130]\n",
      "loss: 0.060149  [347200/367130]\n",
      "loss: 0.179551  [348800/367130]\n",
      "loss: 0.204816  [350400/367130]\n",
      "loss: 0.058799  [352000/367130]\n",
      "loss: 0.144695  [353600/367130]\n",
      "loss: 0.187667  [355200/367130]\n",
      "loss: 0.458944  [356800/367130]\n",
      "loss: 0.418227  [358400/367130]\n",
      "loss: 0.360833  [360000/367130]\n",
      "loss: 0.054395  [361600/367130]\n",
      "loss: 0.256108  [363200/367130]\n",
      "loss: 0.106099  [364800/367130]\n",
      "loss: 0.025723  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.224887 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.215943  [    0/367130]\n",
      "loss: 0.153453  [ 1600/367130]\n",
      "loss: 0.248409  [ 3200/367130]\n",
      "loss: 0.270728  [ 4800/367130]\n",
      "loss: 0.075672  [ 6400/367130]\n",
      "loss: 0.269534  [ 8000/367130]\n",
      "loss: 0.187723  [ 9600/367130]\n",
      "loss: 0.133524  [11200/367130]\n",
      "loss: 0.176811  [12800/367130]\n",
      "loss: 0.192848  [14400/367130]\n",
      "loss: 0.219263  [16000/367130]\n",
      "loss: 0.389675  [17600/367130]\n",
      "loss: 0.161562  [19200/367130]\n",
      "loss: 0.267357  [20800/367130]\n",
      "loss: 0.309983  [22400/367130]\n",
      "loss: 0.093244  [24000/367130]\n",
      "loss: 0.157966  [25600/367130]\n",
      "loss: 0.114010  [27200/367130]\n",
      "loss: 0.474393  [28800/367130]\n",
      "loss: 0.269667  [30400/367130]\n",
      "loss: 0.267398  [32000/367130]\n",
      "loss: 0.052150  [33600/367130]\n",
      "loss: 0.173199  [35200/367130]\n",
      "loss: 0.302134  [36800/367130]\n",
      "loss: 0.327373  [38400/367130]\n",
      "loss: 0.131993  [40000/367130]\n",
      "loss: 0.171649  [41600/367130]\n",
      "loss: 0.375512  [43200/367130]\n",
      "loss: 0.137405  [44800/367130]\n",
      "loss: 0.174873  [46400/367130]\n",
      "loss: 0.174521  [48000/367130]\n",
      "loss: 0.304582  [49600/367130]\n",
      "loss: 0.057667  [51200/367130]\n",
      "loss: 0.197286  [52800/367130]\n",
      "loss: 0.152520  [54400/367130]\n",
      "loss: 0.332856  [56000/367130]\n",
      "loss: 0.246061  [57600/367130]\n",
      "loss: 0.084259  [59200/367130]\n",
      "loss: 0.133603  [60800/367130]\n",
      "loss: 0.432392  [62400/367130]\n",
      "loss: 0.116361  [64000/367130]\n",
      "loss: 0.334707  [65600/367130]\n",
      "loss: 0.014449  [67200/367130]\n",
      "loss: 0.134323  [68800/367130]\n",
      "loss: 0.572844  [70400/367130]\n",
      "loss: 0.212372  [72000/367130]\n",
      "loss: 0.162059  [73600/367130]\n",
      "loss: 0.189020  [75200/367130]\n",
      "loss: 0.138474  [76800/367130]\n",
      "loss: 0.160198  [78400/367130]\n",
      "loss: 0.194682  [80000/367130]\n",
      "loss: 0.195024  [81600/367130]\n",
      "loss: 0.112594  [83200/367130]\n",
      "loss: 0.387771  [84800/367130]\n",
      "loss: 0.115340  [86400/367130]\n",
      "loss: 0.097343  [88000/367130]\n",
      "loss: 0.121156  [89600/367130]\n",
      "loss: 0.221624  [91200/367130]\n",
      "loss: 0.037324  [92800/367130]\n",
      "loss: 0.245224  [94400/367130]\n",
      "loss: 0.122444  [96000/367130]\n",
      "loss: 0.045043  [97600/367130]\n",
      "loss: 0.127753  [99200/367130]\n",
      "loss: 0.123618  [100800/367130]\n",
      "loss: 0.590579  [102400/367130]\n",
      "loss: 0.102773  [104000/367130]\n",
      "loss: 0.441810  [105600/367130]\n",
      "loss: 0.176053  [107200/367130]\n",
      "loss: 0.256527  [108800/367130]\n",
      "loss: 0.346213  [110400/367130]\n",
      "loss: 0.247468  [112000/367130]\n",
      "loss: 0.290015  [113600/367130]\n",
      "loss: 0.086640  [115200/367130]\n",
      "loss: 0.188945  [116800/367130]\n",
      "loss: 0.195187  [118400/367130]\n",
      "loss: 0.096352  [120000/367130]\n",
      "loss: 0.242423  [121600/367130]\n",
      "loss: 0.365647  [123200/367130]\n",
      "loss: 0.790302  [124800/367130]\n",
      "loss: 0.059439  [126400/367130]\n",
      "loss: 0.353843  [128000/367130]\n",
      "loss: 0.301229  [129600/367130]\n",
      "loss: 0.113251  [131200/367130]\n",
      "loss: 0.153632  [132800/367130]\n",
      "loss: 0.165165  [134400/367130]\n",
      "loss: 0.159444  [136000/367130]\n",
      "loss: 0.097859  [137600/367130]\n",
      "loss: 0.203272  [139200/367130]\n",
      "loss: 0.053265  [140800/367130]\n",
      "loss: 0.455912  [142400/367130]\n",
      "loss: 0.042440  [144000/367130]\n",
      "loss: 0.158272  [145600/367130]\n",
      "loss: 0.233601  [147200/367130]\n",
      "loss: 0.326211  [148800/367130]\n",
      "loss: 0.276516  [150400/367130]\n",
      "loss: 0.306254  [152000/367130]\n",
      "loss: 0.248017  [153600/367130]\n",
      "loss: 0.117006  [155200/367130]\n",
      "loss: 0.785520  [156800/367130]\n",
      "loss: 0.144398  [158400/367130]\n",
      "loss: 0.593356  [160000/367130]\n",
      "loss: 0.111147  [161600/367130]\n",
      "loss: 0.168522  [163200/367130]\n",
      "loss: 0.247987  [164800/367130]\n",
      "loss: 0.150295  [166400/367130]\n",
      "loss: 0.244938  [168000/367130]\n",
      "loss: 0.174494  [169600/367130]\n",
      "loss: 0.097093  [171200/367130]\n",
      "loss: 0.106607  [172800/367130]\n",
      "loss: 0.186039  [174400/367130]\n",
      "loss: 0.322016  [176000/367130]\n",
      "loss: 0.038639  [177600/367130]\n",
      "loss: 0.279364  [179200/367130]\n",
      "loss: 0.142264  [180800/367130]\n",
      "loss: 0.284160  [182400/367130]\n",
      "loss: 0.171745  [184000/367130]\n",
      "loss: 0.132846  [185600/367130]\n",
      "loss: 0.053087  [187200/367130]\n",
      "loss: 0.059359  [188800/367130]\n",
      "loss: 0.564289  [190400/367130]\n",
      "loss: 0.204204  [192000/367130]\n",
      "loss: 0.245190  [193600/367130]\n",
      "loss: 0.163213  [195200/367130]\n",
      "loss: 0.415835  [196800/367130]\n",
      "loss: 0.293610  [198400/367130]\n",
      "loss: 0.111517  [200000/367130]\n",
      "loss: 0.258702  [201600/367130]\n",
      "loss: 0.166195  [203200/367130]\n",
      "loss: 0.392924  [204800/367130]\n",
      "loss: 0.105829  [206400/367130]\n",
      "loss: 0.156134  [208000/367130]\n",
      "loss: 0.261750  [209600/367130]\n",
      "loss: 0.051564  [211200/367130]\n",
      "loss: 0.280512  [212800/367130]\n",
      "loss: 0.117321  [214400/367130]\n",
      "loss: 0.262939  [216000/367130]\n",
      "loss: 0.238773  [217600/367130]\n",
      "loss: 0.226529  [219200/367130]\n",
      "loss: 0.100583  [220800/367130]\n",
      "loss: 0.282087  [222400/367130]\n",
      "loss: 0.136669  [224000/367130]\n",
      "loss: 0.124629  [225600/367130]\n",
      "loss: 0.106446  [227200/367130]\n",
      "loss: 0.055380  [228800/367130]\n",
      "loss: 0.107374  [230400/367130]\n",
      "loss: 0.365078  [232000/367130]\n",
      "loss: 0.120010  [233600/367130]\n",
      "loss: 0.336144  [235200/367130]\n",
      "loss: 0.238317  [236800/367130]\n",
      "loss: 0.155870  [238400/367130]\n",
      "loss: 0.181830  [240000/367130]\n",
      "loss: 0.022964  [241600/367130]\n",
      "loss: 0.082125  [243200/367130]\n",
      "loss: 0.117959  [244800/367130]\n",
      "loss: 0.315752  [246400/367130]\n",
      "loss: 0.263439  [248000/367130]\n",
      "loss: 0.484202  [249600/367130]\n",
      "loss: 0.219728  [251200/367130]\n",
      "loss: 0.107303  [252800/367130]\n",
      "loss: 0.501006  [254400/367130]\n",
      "loss: 0.279209  [256000/367130]\n",
      "loss: 0.254566  [257600/367130]\n",
      "loss: 0.310952  [259200/367130]\n",
      "loss: 0.114764  [260800/367130]\n",
      "loss: 0.184994  [262400/367130]\n",
      "loss: 0.385947  [264000/367130]\n",
      "loss: 0.180087  [265600/367130]\n",
      "loss: 0.245481  [267200/367130]\n",
      "loss: 0.058391  [268800/367130]\n",
      "loss: 0.166168  [270400/367130]\n",
      "loss: 0.185350  [272000/367130]\n",
      "loss: 0.590678  [273600/367130]\n",
      "loss: 0.032745  [275200/367130]\n",
      "loss: 0.242941  [276800/367130]\n",
      "loss: 0.301662  [278400/367130]\n",
      "loss: 0.093753  [280000/367130]\n",
      "loss: 0.210647  [281600/367130]\n",
      "loss: 0.053407  [283200/367130]\n",
      "loss: 0.056751  [284800/367130]\n",
      "loss: 0.153635  [286400/367130]\n",
      "loss: 0.070338  [288000/367130]\n",
      "loss: 0.396205  [289600/367130]\n",
      "loss: 0.405017  [291200/367130]\n",
      "loss: 0.088071  [292800/367130]\n",
      "loss: 0.081491  [294400/367130]\n",
      "loss: 0.070905  [296000/367130]\n",
      "loss: 0.389958  [297600/367130]\n",
      "loss: 0.039246  [299200/367130]\n",
      "loss: 0.468525  [300800/367130]\n",
      "loss: 0.373816  [302400/367130]\n",
      "loss: 0.109093  [304000/367130]\n",
      "loss: 0.217969  [305600/367130]\n",
      "loss: 0.111049  [307200/367130]\n",
      "loss: 0.331387  [308800/367130]\n",
      "loss: 0.128004  [310400/367130]\n",
      "loss: 0.244021  [312000/367130]\n",
      "loss: 0.209252  [313600/367130]\n",
      "loss: 0.080442  [315200/367130]\n",
      "loss: 0.453786  [316800/367130]\n",
      "loss: 0.236787  [318400/367130]\n",
      "loss: 0.135805  [320000/367130]\n",
      "loss: 0.131827  [321600/367130]\n",
      "loss: 0.200126  [323200/367130]\n",
      "loss: 0.166302  [324800/367130]\n",
      "loss: 0.337021  [326400/367130]\n",
      "loss: 0.192411  [328000/367130]\n",
      "loss: 0.489765  [329600/367130]\n",
      "loss: 0.244468  [331200/367130]\n",
      "loss: 0.055833  [332800/367130]\n",
      "loss: 0.144464  [334400/367130]\n",
      "loss: 0.297715  [336000/367130]\n",
      "loss: 0.087838  [337600/367130]\n",
      "loss: 0.225285  [339200/367130]\n",
      "loss: 0.106051  [340800/367130]\n",
      "loss: 0.195545  [342400/367130]\n",
      "loss: 0.182441  [344000/367130]\n",
      "loss: 0.237738  [345600/367130]\n",
      "loss: 0.235586  [347200/367130]\n",
      "loss: 0.283658  [348800/367130]\n",
      "loss: 0.113495  [350400/367130]\n",
      "loss: 0.110619  [352000/367130]\n",
      "loss: 0.215084  [353600/367130]\n",
      "loss: 0.268051  [355200/367130]\n",
      "loss: 0.290728  [356800/367130]\n",
      "loss: 0.121846  [358400/367130]\n",
      "loss: 0.264175  [360000/367130]\n",
      "loss: 0.280654  [361600/367130]\n",
      "loss: 0.155916  [363200/367130]\n",
      "loss: 0.174020  [364800/367130]\n",
      "loss: 0.437552  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.223430 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.274020  [    0/367130]\n",
      "loss: 0.203893  [ 1600/367130]\n",
      "loss: 0.277972  [ 3200/367130]\n",
      "loss: 0.234725  [ 4800/367130]\n",
      "loss: 0.161708  [ 6400/367130]\n",
      "loss: 0.279545  [ 8000/367130]\n",
      "loss: 0.145255  [ 9600/367130]\n",
      "loss: 0.296715  [11200/367130]\n",
      "loss: 0.241435  [12800/367130]\n",
      "loss: 0.180448  [14400/367130]\n",
      "loss: 0.103833  [16000/367130]\n",
      "loss: 0.497011  [17600/367130]\n",
      "loss: 0.193167  [19200/367130]\n",
      "loss: 0.212810  [20800/367130]\n",
      "loss: 0.075861  [22400/367130]\n",
      "loss: 0.342404  [24000/367130]\n",
      "loss: 0.225676  [25600/367130]\n",
      "loss: 0.205889  [27200/367130]\n",
      "loss: 0.372941  [28800/367130]\n",
      "loss: 0.162543  [30400/367130]\n",
      "loss: 0.227450  [32000/367130]\n",
      "loss: 0.503102  [33600/367130]\n",
      "loss: 0.145807  [35200/367130]\n",
      "loss: 0.261613  [36800/367130]\n",
      "loss: 0.129482  [38400/367130]\n",
      "loss: 0.564351  [40000/367130]\n",
      "loss: 0.454325  [41600/367130]\n",
      "loss: 0.177774  [43200/367130]\n",
      "loss: 0.480550  [44800/367130]\n",
      "loss: 0.145453  [46400/367130]\n",
      "loss: 0.274283  [48000/367130]\n",
      "loss: 0.445854  [49600/367130]\n",
      "loss: 0.041928  [51200/367130]\n",
      "loss: 0.063186  [52800/367130]\n",
      "loss: 0.160530  [54400/367130]\n",
      "loss: 0.170091  [56000/367130]\n",
      "loss: 0.106710  [57600/367130]\n",
      "loss: 0.192285  [59200/367130]\n",
      "loss: 0.301910  [60800/367130]\n",
      "loss: 0.180528  [62400/367130]\n",
      "loss: 0.276537  [64000/367130]\n",
      "loss: 0.131980  [65600/367130]\n",
      "loss: 0.074300  [67200/367130]\n",
      "loss: 0.149114  [68800/367130]\n",
      "loss: 0.198164  [70400/367130]\n",
      "loss: 0.140866  [72000/367130]\n",
      "loss: 0.290361  [73600/367130]\n",
      "loss: 0.177197  [75200/367130]\n",
      "loss: 0.106917  [76800/367130]\n",
      "loss: 0.190500  [78400/367130]\n",
      "loss: 0.536397  [80000/367130]\n",
      "loss: 0.297284  [81600/367130]\n",
      "loss: 0.476940  [83200/367130]\n",
      "loss: 0.452002  [84800/367130]\n",
      "loss: 0.236830  [86400/367130]\n",
      "loss: 0.338055  [88000/367130]\n",
      "loss: 0.370398  [89600/367130]\n",
      "loss: 0.177534  [91200/367130]\n",
      "loss: 0.058654  [92800/367130]\n",
      "loss: 0.520682  [94400/367130]\n",
      "loss: 0.092091  [96000/367130]\n",
      "loss: 0.129335  [97600/367130]\n",
      "loss: 0.211799  [99200/367130]\n",
      "loss: 0.148999  [100800/367130]\n",
      "loss: 0.167218  [102400/367130]\n",
      "loss: 0.106673  [104000/367130]\n",
      "loss: 0.254508  [105600/367130]\n",
      "loss: 0.270027  [107200/367130]\n",
      "loss: 0.319779  [108800/367130]\n",
      "loss: 0.101113  [110400/367130]\n",
      "loss: 0.217162  [112000/367130]\n",
      "loss: 0.474674  [113600/367130]\n",
      "loss: 0.202518  [115200/367130]\n",
      "loss: 0.094120  [116800/367130]\n",
      "loss: 0.127999  [118400/367130]\n",
      "loss: 0.110707  [120000/367130]\n",
      "loss: 0.139592  [121600/367130]\n",
      "loss: 0.199592  [123200/367130]\n",
      "loss: 0.104381  [124800/367130]\n",
      "loss: 0.204474  [126400/367130]\n",
      "loss: 0.219490  [128000/367130]\n",
      "loss: 0.078929  [129600/367130]\n",
      "loss: 0.111965  [131200/367130]\n",
      "loss: 0.295042  [132800/367130]\n",
      "loss: 0.092811  [134400/367130]\n",
      "loss: 0.021740  [136000/367130]\n",
      "loss: 0.133648  [137600/367130]\n",
      "loss: 0.272503  [139200/367130]\n",
      "loss: 0.176568  [140800/367130]\n",
      "loss: 0.497027  [142400/367130]\n",
      "loss: 0.155513  [144000/367130]\n",
      "loss: 0.437755  [145600/367130]\n",
      "loss: 0.151346  [147200/367130]\n",
      "loss: 0.140995  [148800/367130]\n",
      "loss: 0.401209  [150400/367130]\n",
      "loss: 0.416752  [152000/367130]\n",
      "loss: 0.145814  [153600/367130]\n",
      "loss: 0.314743  [155200/367130]\n",
      "loss: 0.091683  [156800/367130]\n",
      "loss: 0.108121  [158400/367130]\n",
      "loss: 0.177520  [160000/367130]\n",
      "loss: 0.154483  [161600/367130]\n",
      "loss: 0.095655  [163200/367130]\n",
      "loss: 0.416188  [164800/367130]\n",
      "loss: 0.198590  [166400/367130]\n",
      "loss: 0.083158  [168000/367130]\n",
      "loss: 0.071514  [169600/367130]\n",
      "loss: 0.085459  [171200/367130]\n",
      "loss: 0.162742  [172800/367130]\n",
      "loss: 0.188859  [174400/367130]\n",
      "loss: 0.203441  [176000/367130]\n",
      "loss: 0.125011  [177600/367130]\n",
      "loss: 0.069186  [179200/367130]\n",
      "loss: 0.071944  [180800/367130]\n",
      "loss: 0.135003  [182400/367130]\n",
      "loss: 0.236168  [184000/367130]\n",
      "loss: 0.134586  [185600/367130]\n",
      "loss: 0.156986  [187200/367130]\n",
      "loss: 0.246093  [188800/367130]\n",
      "loss: 0.178388  [190400/367130]\n",
      "loss: 0.076127  [192000/367130]\n",
      "loss: 0.246097  [193600/367130]\n",
      "loss: 0.265441  [195200/367130]\n",
      "loss: 0.167696  [196800/367130]\n",
      "loss: 0.369149  [198400/367130]\n",
      "loss: 0.352095  [200000/367130]\n",
      "loss: 0.414333  [201600/367130]\n",
      "loss: 0.132039  [203200/367130]\n",
      "loss: 0.219825  [204800/367130]\n",
      "loss: 0.602320  [206400/367130]\n",
      "loss: 0.326339  [208000/367130]\n",
      "loss: 0.141533  [209600/367130]\n",
      "loss: 0.156180  [211200/367130]\n",
      "loss: 0.202638  [212800/367130]\n",
      "loss: 0.285934  [214400/367130]\n",
      "loss: 0.439843  [216000/367130]\n",
      "loss: 0.127963  [217600/367130]\n",
      "loss: 0.105169  [219200/367130]\n",
      "loss: 0.281212  [220800/367130]\n",
      "loss: 0.325384  [222400/367130]\n",
      "loss: 0.083461  [224000/367130]\n",
      "loss: 0.117085  [225600/367130]\n",
      "loss: 0.226734  [227200/367130]\n",
      "loss: 0.133751  [228800/367130]\n",
      "loss: 0.119802  [230400/367130]\n",
      "loss: 0.370186  [232000/367130]\n",
      "loss: 0.304373  [233600/367130]\n",
      "loss: 0.260258  [235200/367130]\n",
      "loss: 0.189642  [236800/367130]\n",
      "loss: 0.195533  [238400/367130]\n",
      "loss: 0.452766  [240000/367130]\n",
      "loss: 0.407912  [241600/367130]\n",
      "loss: 0.093570  [243200/367130]\n",
      "loss: 0.232391  [244800/367130]\n",
      "loss: 0.054379  [246400/367130]\n",
      "loss: 0.687536  [248000/367130]\n",
      "loss: 0.392716  [249600/367130]\n",
      "loss: 0.250046  [251200/367130]\n",
      "loss: 0.158934  [252800/367130]\n",
      "loss: 0.103754  [254400/367130]\n",
      "loss: 0.153405  [256000/367130]\n",
      "loss: 0.131645  [257600/367130]\n",
      "loss: 0.133600  [259200/367130]\n",
      "loss: 0.238360  [260800/367130]\n",
      "loss: 0.172451  [262400/367130]\n",
      "loss: 0.214901  [264000/367130]\n",
      "loss: 0.101941  [265600/367130]\n",
      "loss: 0.084659  [267200/367130]\n",
      "loss: 0.203204  [268800/367130]\n",
      "loss: 0.292437  [270400/367130]\n",
      "loss: 0.156408  [272000/367130]\n",
      "loss: 0.134140  [273600/367130]\n",
      "loss: 0.229056  [275200/367130]\n",
      "loss: 0.162578  [276800/367130]\n",
      "loss: 0.422139  [278400/367130]\n",
      "loss: 0.156890  [280000/367130]\n",
      "loss: 0.171582  [281600/367130]\n",
      "loss: 0.211763  [283200/367130]\n",
      "loss: 0.440563  [284800/367130]\n",
      "loss: 0.383739  [286400/367130]\n",
      "loss: 0.066728  [288000/367130]\n",
      "loss: 0.091591  [289600/367130]\n",
      "loss: 0.151582  [291200/367130]\n",
      "loss: 0.134781  [292800/367130]\n",
      "loss: 0.220050  [294400/367130]\n",
      "loss: 0.121204  [296000/367130]\n",
      "loss: 0.282115  [297600/367130]\n",
      "loss: 0.465089  [299200/367130]\n",
      "loss: 0.220651  [300800/367130]\n",
      "loss: 0.107070  [302400/367130]\n",
      "loss: 0.164940  [304000/367130]\n",
      "loss: 0.095009  [305600/367130]\n",
      "loss: 0.130117  [307200/367130]\n",
      "loss: 0.361269  [308800/367130]\n",
      "loss: 0.156363  [310400/367130]\n",
      "loss: 0.209676  [312000/367130]\n",
      "loss: 0.184056  [313600/367130]\n",
      "loss: 0.138424  [315200/367130]\n",
      "loss: 0.128625  [316800/367130]\n",
      "loss: 0.100591  [318400/367130]\n",
      "loss: 0.475459  [320000/367130]\n",
      "loss: 0.305848  [321600/367130]\n",
      "loss: 0.152828  [323200/367130]\n",
      "loss: 0.315595  [324800/367130]\n",
      "loss: 0.093962  [326400/367130]\n",
      "loss: 0.139925  [328000/367130]\n",
      "loss: 0.363475  [329600/367130]\n",
      "loss: 0.181411  [331200/367130]\n",
      "loss: 0.293822  [332800/367130]\n",
      "loss: 0.162631  [334400/367130]\n",
      "loss: 0.253549  [336000/367130]\n",
      "loss: 0.196523  [337600/367130]\n",
      "loss: 0.271676  [339200/367130]\n",
      "loss: 0.346298  [340800/367130]\n",
      "loss: 0.062720  [342400/367130]\n",
      "loss: 0.335144  [344000/367130]\n",
      "loss: 0.253794  [345600/367130]\n",
      "loss: 0.280864  [347200/367130]\n",
      "loss: 0.309762  [348800/367130]\n",
      "loss: 0.123443  [350400/367130]\n",
      "loss: 0.182461  [352000/367130]\n",
      "loss: 0.154765  [353600/367130]\n",
      "loss: 0.196290  [355200/367130]\n",
      "loss: 0.152687  [356800/367130]\n",
      "loss: 0.054186  [358400/367130]\n",
      "loss: 0.282139  [360000/367130]\n",
      "loss: 0.359784  [361600/367130]\n",
      "loss: 0.155481  [363200/367130]\n",
      "loss: 0.208100  [364800/367130]\n",
      "loss: 0.071668  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.222922 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.066669  [    0/367130]\n",
      "loss: 0.258944  [ 1600/367130]\n",
      "loss: 0.432144  [ 3200/367130]\n",
      "loss: 0.116417  [ 4800/367130]\n",
      "loss: 0.164705  [ 6400/367130]\n",
      "loss: 0.236769  [ 8000/367130]\n",
      "loss: 0.185616  [ 9600/367130]\n",
      "loss: 0.250398  [11200/367130]\n",
      "loss: 0.260443  [12800/367130]\n",
      "loss: 0.440932  [14400/367130]\n",
      "loss: 0.126143  [16000/367130]\n",
      "loss: 0.107377  [17600/367130]\n",
      "loss: 0.045893  [19200/367130]\n",
      "loss: 0.069553  [20800/367130]\n",
      "loss: 0.282373  [22400/367130]\n",
      "loss: 0.147121  [24000/367130]\n",
      "loss: 0.069857  [25600/367130]\n",
      "loss: 0.166022  [27200/367130]\n",
      "loss: 0.216171  [28800/367130]\n",
      "loss: 0.143605  [30400/367130]\n",
      "loss: 0.202009  [32000/367130]\n",
      "loss: 0.189524  [33600/367130]\n",
      "loss: 0.066824  [35200/367130]\n",
      "loss: 0.213691  [36800/367130]\n",
      "loss: 0.338721  [38400/367130]\n",
      "loss: 0.170630  [40000/367130]\n",
      "loss: 0.055040  [41600/367130]\n",
      "loss: 0.489978  [43200/367130]\n",
      "loss: 0.340473  [44800/367130]\n",
      "loss: 0.266340  [46400/367130]\n",
      "loss: 0.402167  [48000/367130]\n",
      "loss: 0.130299  [49600/367130]\n",
      "loss: 0.278754  [51200/367130]\n",
      "loss: 0.113904  [52800/367130]\n",
      "loss: 0.285062  [54400/367130]\n",
      "loss: 0.411624  [56000/367130]\n",
      "loss: 0.397334  [57600/367130]\n",
      "loss: 0.104483  [59200/367130]\n",
      "loss: 0.138518  [60800/367130]\n",
      "loss: 0.164771  [62400/367130]\n",
      "loss: 0.082398  [64000/367130]\n",
      "loss: 0.118584  [65600/367130]\n",
      "loss: 0.206368  [67200/367130]\n",
      "loss: 0.262882  [68800/367130]\n",
      "loss: 0.411529  [70400/367130]\n",
      "loss: 0.342133  [72000/367130]\n",
      "loss: 0.552798  [73600/367130]\n",
      "loss: 0.290513  [75200/367130]\n",
      "loss: 0.094030  [76800/367130]\n",
      "loss: 0.281703  [78400/367130]\n",
      "loss: 0.205094  [80000/367130]\n",
      "loss: 0.360052  [81600/367130]\n",
      "loss: 0.129896  [83200/367130]\n",
      "loss: 0.217725  [84800/367130]\n",
      "loss: 0.297133  [86400/367130]\n",
      "loss: 0.166084  [88000/367130]\n",
      "loss: 0.366587  [89600/367130]\n",
      "loss: 0.182784  [91200/367130]\n",
      "loss: 0.135549  [92800/367130]\n",
      "loss: 0.118745  [94400/367130]\n",
      "loss: 0.206626  [96000/367130]\n",
      "loss: 0.197292  [97600/367130]\n",
      "loss: 0.098221  [99200/367130]\n",
      "loss: 0.152768  [100800/367130]\n",
      "loss: 0.177227  [102400/367130]\n",
      "loss: 0.373978  [104000/367130]\n",
      "loss: 0.280392  [105600/367130]\n",
      "loss: 0.307811  [107200/367130]\n",
      "loss: 0.127363  [108800/367130]\n",
      "loss: 0.023066  [110400/367130]\n",
      "loss: 0.170990  [112000/367130]\n",
      "loss: 0.098603  [113600/367130]\n",
      "loss: 0.114769  [115200/367130]\n",
      "loss: 0.070255  [116800/367130]\n",
      "loss: 0.170630  [118400/367130]\n",
      "loss: 0.306920  [120000/367130]\n",
      "loss: 0.062119  [121600/367130]\n",
      "loss: 0.096971  [123200/367130]\n",
      "loss: 0.251378  [124800/367130]\n",
      "loss: 0.150678  [126400/367130]\n",
      "loss: 0.184925  [128000/367130]\n",
      "loss: 0.338243  [129600/367130]\n",
      "loss: 0.175874  [131200/367130]\n",
      "loss: 0.288534  [132800/367130]\n",
      "loss: 0.343320  [134400/367130]\n",
      "loss: 0.433583  [136000/367130]\n",
      "loss: 0.355555  [137600/367130]\n",
      "loss: 0.199966  [139200/367130]\n",
      "loss: 0.109552  [140800/367130]\n",
      "loss: 0.096841  [142400/367130]\n",
      "loss: 0.287516  [144000/367130]\n",
      "loss: 0.298040  [145600/367130]\n",
      "loss: 0.109634  [147200/367130]\n",
      "loss: 0.276293  [148800/367130]\n",
      "loss: 0.138441  [150400/367130]\n",
      "loss: 0.185341  [152000/367130]\n",
      "loss: 0.140481  [153600/367130]\n",
      "loss: 0.323771  [155200/367130]\n",
      "loss: 0.245031  [156800/367130]\n",
      "loss: 0.397453  [158400/367130]\n",
      "loss: 0.312827  [160000/367130]\n",
      "loss: 0.069274  [161600/367130]\n",
      "loss: 0.334594  [163200/367130]\n",
      "loss: 0.129294  [164800/367130]\n",
      "loss: 0.160080  [166400/367130]\n",
      "loss: 0.172948  [168000/367130]\n",
      "loss: 0.183593  [169600/367130]\n",
      "loss: 0.204682  [171200/367130]\n",
      "loss: 0.333554  [172800/367130]\n",
      "loss: 0.063389  [174400/367130]\n",
      "loss: 0.103052  [176000/367130]\n",
      "loss: 0.213714  [177600/367130]\n",
      "loss: 0.199138  [179200/367130]\n",
      "loss: 0.262824  [180800/367130]\n",
      "loss: 0.377768  [182400/367130]\n",
      "loss: 0.076284  [184000/367130]\n",
      "loss: 0.271615  [185600/367130]\n",
      "loss: 0.145189  [187200/367130]\n",
      "loss: 0.389346  [188800/367130]\n",
      "loss: 0.126102  [190400/367130]\n",
      "loss: 0.251349  [192000/367130]\n",
      "loss: 0.180422  [193600/367130]\n",
      "loss: 0.474191  [195200/367130]\n",
      "loss: 0.140839  [196800/367130]\n",
      "loss: 0.167126  [198400/367130]\n",
      "loss: 0.846475  [200000/367130]\n",
      "loss: 0.340726  [201600/367130]\n",
      "loss: 0.087096  [203200/367130]\n",
      "loss: 0.247637  [204800/367130]\n",
      "loss: 0.139030  [206400/367130]\n",
      "loss: 0.326059  [208000/367130]\n",
      "loss: 0.144322  [209600/367130]\n",
      "loss: 0.112443  [211200/367130]\n",
      "loss: 0.151909  [212800/367130]\n",
      "loss: 0.173098  [214400/367130]\n",
      "loss: 0.052998  [216000/367130]\n",
      "loss: 0.240454  [217600/367130]\n",
      "loss: 0.121089  [219200/367130]\n",
      "loss: 0.035683  [220800/367130]\n",
      "loss: 0.148781  [222400/367130]\n",
      "loss: 0.254155  [224000/367130]\n",
      "loss: 0.194780  [225600/367130]\n",
      "loss: 0.183440  [227200/367130]\n",
      "loss: 0.132837  [228800/367130]\n",
      "loss: 0.262565  [230400/367130]\n",
      "loss: 0.262392  [232000/367130]\n",
      "loss: 0.053001  [233600/367130]\n",
      "loss: 0.387031  [235200/367130]\n",
      "loss: 0.361189  [236800/367130]\n",
      "loss: 0.054806  [238400/367130]\n",
      "loss: 0.186475  [240000/367130]\n",
      "loss: 0.262159  [241600/367130]\n",
      "loss: 0.394030  [243200/367130]\n",
      "loss: 0.294533  [244800/367130]\n",
      "loss: 0.269796  [246400/367130]\n",
      "loss: 0.083217  [248000/367130]\n",
      "loss: 0.114249  [249600/367130]\n",
      "loss: 0.164940  [251200/367130]\n",
      "loss: 0.096524  [252800/367130]\n",
      "loss: 0.088035  [254400/367130]\n",
      "loss: 0.179478  [256000/367130]\n",
      "loss: 0.090986  [257600/367130]\n",
      "loss: 0.229004  [259200/367130]\n",
      "loss: 0.136678  [260800/367130]\n",
      "loss: 0.113396  [262400/367130]\n",
      "loss: 0.053384  [264000/367130]\n",
      "loss: 0.268948  [265600/367130]\n",
      "loss: 0.056481  [267200/367130]\n",
      "loss: 0.309209  [268800/367130]\n",
      "loss: 0.424675  [270400/367130]\n",
      "loss: 0.283049  [272000/367130]\n",
      "loss: 0.118089  [273600/367130]\n",
      "loss: 0.170725  [275200/367130]\n",
      "loss: 0.246205  [276800/367130]\n",
      "loss: 0.165494  [278400/367130]\n",
      "loss: 0.162700  [280000/367130]\n",
      "loss: 0.376100  [281600/367130]\n",
      "loss: 0.196581  [283200/367130]\n",
      "loss: 0.322040  [284800/367130]\n",
      "loss: 0.069952  [286400/367130]\n",
      "loss: 0.270135  [288000/367130]\n",
      "loss: 0.375644  [289600/367130]\n",
      "loss: 0.063976  [291200/367130]\n",
      "loss: 0.151195  [292800/367130]\n",
      "loss: 0.062115  [294400/367130]\n",
      "loss: 0.223226  [296000/367130]\n",
      "loss: 0.154535  [297600/367130]\n",
      "loss: 0.093756  [299200/367130]\n",
      "loss: 0.112029  [300800/367130]\n",
      "loss: 0.329778  [302400/367130]\n",
      "loss: 0.135901  [304000/367130]\n",
      "loss: 0.392788  [305600/367130]\n",
      "loss: 0.234394  [307200/367130]\n",
      "loss: 0.278993  [308800/367130]\n",
      "loss: 0.742144  [310400/367130]\n",
      "loss: 0.112482  [312000/367130]\n",
      "loss: 0.280629  [313600/367130]\n",
      "loss: 0.071304  [315200/367130]\n",
      "loss: 0.320850  [316800/367130]\n",
      "loss: 0.114977  [318400/367130]\n",
      "loss: 0.203578  [320000/367130]\n",
      "loss: 0.159916  [321600/367130]\n",
      "loss: 0.116742  [323200/367130]\n",
      "loss: 0.169583  [324800/367130]\n",
      "loss: 0.271483  [326400/367130]\n",
      "loss: 0.328013  [328000/367130]\n",
      "loss: 0.248357  [329600/367130]\n",
      "loss: 0.214881  [331200/367130]\n",
      "loss: 0.164450  [332800/367130]\n",
      "loss: 0.125890  [334400/367130]\n",
      "loss: 0.183149  [336000/367130]\n",
      "loss: 0.177770  [337600/367130]\n",
      "loss: 0.386935  [339200/367130]\n",
      "loss: 0.161911  [340800/367130]\n",
      "loss: 0.044748  [342400/367130]\n",
      "loss: 0.112614  [344000/367130]\n",
      "loss: 0.109863  [345600/367130]\n",
      "loss: 0.239911  [347200/367130]\n",
      "loss: 0.091247  [348800/367130]\n",
      "loss: 0.752059  [350400/367130]\n",
      "loss: 0.316470  [352000/367130]\n",
      "loss: 0.680135  [353600/367130]\n",
      "loss: 0.211921  [355200/367130]\n",
      "loss: 0.135880  [356800/367130]\n",
      "loss: 0.298522  [358400/367130]\n",
      "loss: 0.290721  [360000/367130]\n",
      "loss: 0.155267  [361600/367130]\n",
      "loss: 0.021611  [363200/367130]\n",
      "loss: 0.437232  [364800/367130]\n",
      "loss: 0.206603  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.222988 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.443258  [    0/367130]\n",
      "loss: 0.286101  [ 1600/367130]\n",
      "loss: 0.111540  [ 3200/367130]\n",
      "loss: 0.332307  [ 4800/367130]\n",
      "loss: 0.151073  [ 6400/367130]\n",
      "loss: 0.236330  [ 8000/367130]\n",
      "loss: 0.174703  [ 9600/367130]\n",
      "loss: 0.059228  [11200/367130]\n",
      "loss: 0.131722  [12800/367130]\n",
      "loss: 0.133369  [14400/367130]\n",
      "loss: 0.124576  [16000/367130]\n",
      "loss: 0.444376  [17600/367130]\n",
      "loss: 0.244485  [19200/367130]\n",
      "loss: 0.448254  [20800/367130]\n",
      "loss: 0.036226  [22400/367130]\n",
      "loss: 0.503444  [24000/367130]\n",
      "loss: 0.176520  [25600/367130]\n",
      "loss: 0.202673  [27200/367130]\n",
      "loss: 0.221468  [28800/367130]\n",
      "loss: 0.088428  [30400/367130]\n",
      "loss: 0.270005  [32000/367130]\n",
      "loss: 0.234683  [33600/367130]\n",
      "loss: 0.080956  [35200/367130]\n",
      "loss: 0.275359  [36800/367130]\n",
      "loss: 0.139916  [38400/367130]\n",
      "loss: 0.097563  [40000/367130]\n",
      "loss: 0.152582  [41600/367130]\n",
      "loss: 0.256132  [43200/367130]\n",
      "loss: 0.149222  [44800/367130]\n",
      "loss: 0.185037  [46400/367130]\n",
      "loss: 0.163751  [48000/367130]\n",
      "loss: 0.230735  [49600/367130]\n",
      "loss: 0.167561  [51200/367130]\n",
      "loss: 0.278962  [52800/367130]\n",
      "loss: 0.366970  [54400/367130]\n",
      "loss: 0.258110  [56000/367130]\n",
      "loss: 0.197465  [57600/367130]\n",
      "loss: 0.577848  [59200/367130]\n",
      "loss: 0.079299  [60800/367130]\n",
      "loss: 0.101414  [62400/367130]\n",
      "loss: 0.234028  [64000/367130]\n",
      "loss: 0.191141  [65600/367130]\n",
      "loss: 0.268526  [67200/367130]\n",
      "loss: 0.125376  [68800/367130]\n",
      "loss: 0.449331  [70400/367130]\n",
      "loss: 0.191583  [72000/367130]\n",
      "loss: 0.133256  [73600/367130]\n",
      "loss: 0.054293  [75200/367130]\n",
      "loss: 0.261704  [76800/367130]\n",
      "loss: 0.186963  [78400/367130]\n",
      "loss: 0.116263  [80000/367130]\n",
      "loss: 0.257458  [81600/367130]\n",
      "loss: 0.051945  [83200/367130]\n",
      "loss: 0.045498  [84800/367130]\n",
      "loss: 0.182517  [86400/367130]\n",
      "loss: 0.332609  [88000/367130]\n",
      "loss: 0.123693  [89600/367130]\n",
      "loss: 0.199994  [91200/367130]\n",
      "loss: 0.094644  [92800/367130]\n",
      "loss: 0.348930  [94400/367130]\n",
      "loss: 0.090495  [96000/367130]\n",
      "loss: 0.167505  [97600/367130]\n",
      "loss: 0.192529  [99200/367130]\n",
      "loss: 0.187510  [100800/367130]\n",
      "loss: 0.129433  [102400/367130]\n",
      "loss: 0.343506  [104000/367130]\n",
      "loss: 0.175163  [105600/367130]\n",
      "loss: 0.080657  [107200/367130]\n",
      "loss: 0.176247  [108800/367130]\n",
      "loss: 0.069950  [110400/367130]\n",
      "loss: 0.189577  [112000/367130]\n",
      "loss: 0.159063  [113600/367130]\n",
      "loss: 0.192649  [115200/367130]\n",
      "loss: 0.235857  [116800/367130]\n",
      "loss: 0.168347  [118400/367130]\n",
      "loss: 0.250447  [120000/367130]\n",
      "loss: 0.239543  [121600/367130]\n",
      "loss: 0.181495  [123200/367130]\n",
      "loss: 0.376214  [124800/367130]\n",
      "loss: 0.290713  [126400/367130]\n",
      "loss: 0.066780  [128000/367130]\n",
      "loss: 0.424555  [129600/367130]\n",
      "loss: 0.067382  [131200/367130]\n",
      "loss: 0.056555  [132800/367130]\n",
      "loss: 0.078985  [134400/367130]\n",
      "loss: 0.080606  [136000/367130]\n",
      "loss: 0.150220  [137600/367130]\n",
      "loss: 0.174909  [139200/367130]\n",
      "loss: 0.096899  [140800/367130]\n",
      "loss: 0.222009  [142400/367130]\n",
      "loss: 0.250759  [144000/367130]\n",
      "loss: 0.374372  [145600/367130]\n",
      "loss: 0.129513  [147200/367130]\n",
      "loss: 0.140037  [148800/367130]\n",
      "loss: 0.306555  [150400/367130]\n",
      "loss: 0.433067  [152000/367130]\n",
      "loss: 0.307267  [153600/367130]\n",
      "loss: 0.038899  [155200/367130]\n",
      "loss: 0.179438  [156800/367130]\n",
      "loss: 0.137163  [158400/367130]\n",
      "loss: 0.043050  [160000/367130]\n",
      "loss: 0.167136  [161600/367130]\n",
      "loss: 0.163421  [163200/367130]\n",
      "loss: 0.114584  [164800/367130]\n",
      "loss: 0.539289  [166400/367130]\n",
      "loss: 0.176668  [168000/367130]\n",
      "loss: 0.187087  [169600/367130]\n",
      "loss: 0.314490  [171200/367130]\n",
      "loss: 0.220576  [172800/367130]\n",
      "loss: 0.132146  [174400/367130]\n",
      "loss: 0.164805  [176000/367130]\n",
      "loss: 0.122578  [177600/367130]\n",
      "loss: 0.182817  [179200/367130]\n",
      "loss: 0.320039  [180800/367130]\n",
      "loss: 0.115157  [182400/367130]\n",
      "loss: 0.053453  [184000/367130]\n",
      "loss: 0.066425  [185600/367130]\n",
      "loss: 0.172174  [187200/367130]\n",
      "loss: 0.166479  [188800/367130]\n",
      "loss: 0.061981  [190400/367130]\n",
      "loss: 0.045101  [192000/367130]\n",
      "loss: 0.250699  [193600/367130]\n",
      "loss: 0.114478  [195200/367130]\n",
      "loss: 0.434383  [196800/367130]\n",
      "loss: 0.334995  [198400/367130]\n",
      "loss: 0.123347  [200000/367130]\n",
      "loss: 0.201512  [201600/367130]\n",
      "loss: 0.211812  [203200/367130]\n",
      "loss: 0.189014  [204800/367130]\n",
      "loss: 0.092329  [206400/367130]\n",
      "loss: 0.317158  [208000/367130]\n",
      "loss: 0.422036  [209600/367130]\n",
      "loss: 0.224629  [211200/367130]\n",
      "loss: 0.308913  [212800/367130]\n",
      "loss: 0.346103  [214400/367130]\n",
      "loss: 0.051077  [216000/367130]\n",
      "loss: 0.237503  [217600/367130]\n",
      "loss: 0.177415  [219200/367130]\n",
      "loss: 0.305408  [220800/367130]\n",
      "loss: 0.205902  [222400/367130]\n",
      "loss: 0.100970  [224000/367130]\n",
      "loss: 0.283280  [225600/367130]\n",
      "loss: 0.214824  [227200/367130]\n",
      "loss: 0.308450  [228800/367130]\n",
      "loss: 0.169157  [230400/367130]\n",
      "loss: 0.105495  [232000/367130]\n",
      "loss: 0.116429  [233600/367130]\n",
      "loss: 0.479940  [235200/367130]\n",
      "loss: 0.238350  [236800/367130]\n",
      "loss: 0.219293  [238400/367130]\n",
      "loss: 0.088989  [240000/367130]\n",
      "loss: 0.356334  [241600/367130]\n",
      "loss: 0.166746  [243200/367130]\n",
      "loss: 0.254254  [244800/367130]\n",
      "loss: 0.287723  [246400/367130]\n",
      "loss: 0.040617  [248000/367130]\n",
      "loss: 0.257902  [249600/367130]\n",
      "loss: 0.458640  [251200/367130]\n",
      "loss: 0.395970  [252800/367130]\n",
      "loss: 0.234537  [254400/367130]\n",
      "loss: 0.019519  [256000/367130]\n",
      "loss: 0.079426  [257600/367130]\n",
      "loss: 0.266682  [259200/367130]\n",
      "loss: 0.108421  [260800/367130]\n",
      "loss: 0.179545  [262400/367130]\n",
      "loss: 0.071434  [264000/367130]\n",
      "loss: 0.160824  [265600/367130]\n",
      "loss: 0.176998  [267200/367130]\n",
      "loss: 0.114060  [268800/367130]\n",
      "loss: 0.139044  [270400/367130]\n",
      "loss: 0.143623  [272000/367130]\n",
      "loss: 0.208591  [273600/367130]\n",
      "loss: 0.207348  [275200/367130]\n",
      "loss: 0.134382  [276800/367130]\n",
      "loss: 0.111511  [278400/367130]\n",
      "loss: 0.375009  [280000/367130]\n",
      "loss: 0.191018  [281600/367130]\n",
      "loss: 0.270378  [283200/367130]\n",
      "loss: 0.281627  [284800/367130]\n",
      "loss: 0.331981  [286400/367130]\n",
      "loss: 0.156583  [288000/367130]\n",
      "loss: 0.048358  [289600/367130]\n",
      "loss: 0.267660  [291200/367130]\n",
      "loss: 0.320920  [292800/367130]\n",
      "loss: 0.033730  [294400/367130]\n",
      "loss: 0.342808  [296000/367130]\n",
      "loss: 0.181940  [297600/367130]\n",
      "loss: 0.142670  [299200/367130]\n",
      "loss: 0.106793  [300800/367130]\n",
      "loss: 0.141725  [302400/367130]\n",
      "loss: 0.310036  [304000/367130]\n",
      "loss: 0.193116  [305600/367130]\n",
      "loss: 0.267699  [307200/367130]\n",
      "loss: 0.161879  [308800/367130]\n",
      "loss: 0.126390  [310400/367130]\n",
      "loss: 0.391332  [312000/367130]\n",
      "loss: 0.274328  [313600/367130]\n",
      "loss: 0.334161  [315200/367130]\n",
      "loss: 0.397920  [316800/367130]\n",
      "loss: 0.290198  [318400/367130]\n",
      "loss: 0.398806  [320000/367130]\n",
      "loss: 0.109911  [321600/367130]\n",
      "loss: 0.061865  [323200/367130]\n",
      "loss: 0.154421  [324800/367130]\n",
      "loss: 0.019621  [326400/367130]\n",
      "loss: 0.145558  [328000/367130]\n",
      "loss: 0.165269  [329600/367130]\n",
      "loss: 0.215956  [331200/367130]\n",
      "loss: 0.399706  [332800/367130]\n",
      "loss: 0.134833  [334400/367130]\n",
      "loss: 0.109111  [336000/367130]\n",
      "loss: 0.321311  [337600/367130]\n",
      "loss: 0.232025  [339200/367130]\n",
      "loss: 0.306517  [340800/367130]\n",
      "loss: 0.149470  [342400/367130]\n",
      "loss: 0.403940  [344000/367130]\n",
      "loss: 0.314297  [345600/367130]\n",
      "loss: 0.308255  [347200/367130]\n",
      "loss: 0.194318  [348800/367130]\n",
      "loss: 0.278373  [350400/367130]\n",
      "loss: 0.172513  [352000/367130]\n",
      "loss: 0.076056  [353600/367130]\n",
      "loss: 0.184852  [355200/367130]\n",
      "loss: 0.094633  [356800/367130]\n",
      "loss: 0.433262  [358400/367130]\n",
      "loss: 0.114259  [360000/367130]\n",
      "loss: 0.113378  [361600/367130]\n",
      "loss: 0.190463  [363200/367130]\n",
      "loss: 0.187625  [364800/367130]\n",
      "loss: 0.133398  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.222934 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.078130  [    0/367130]\n",
      "loss: 0.304302  [ 1600/367130]\n",
      "loss: 0.059350  [ 3200/367130]\n",
      "loss: 0.218776  [ 4800/367130]\n",
      "loss: 0.134875  [ 6400/367130]\n",
      "loss: 0.197573  [ 8000/367130]\n",
      "loss: 0.170943  [ 9600/367130]\n",
      "loss: 0.416540  [11200/367130]\n",
      "loss: 0.188011  [12800/367130]\n",
      "loss: 0.507168  [14400/367130]\n",
      "loss: 0.149255  [16000/367130]\n",
      "loss: 0.154667  [17600/367130]\n",
      "loss: 0.090043  [19200/367130]\n",
      "loss: 0.076400  [20800/367130]\n",
      "loss: 0.186306  [22400/367130]\n",
      "loss: 0.543947  [24000/367130]\n",
      "loss: 0.120248  [25600/367130]\n",
      "loss: 0.509766  [27200/367130]\n",
      "loss: 0.325786  [28800/367130]\n",
      "loss: 0.222082  [30400/367130]\n",
      "loss: 0.125503  [32000/367130]\n",
      "loss: 0.120377  [33600/367130]\n",
      "loss: 0.140811  [35200/367130]\n",
      "loss: 0.108579  [36800/367130]\n",
      "loss: 0.224486  [38400/367130]\n",
      "loss: 0.074655  [40000/367130]\n",
      "loss: 0.110253  [41600/367130]\n",
      "loss: 0.106544  [43200/367130]\n",
      "loss: 0.153125  [44800/367130]\n",
      "loss: 0.161775  [46400/367130]\n",
      "loss: 0.267492  [48000/367130]\n",
      "loss: 0.158186  [49600/367130]\n",
      "loss: 0.464419  [51200/367130]\n",
      "loss: 0.034947  [52800/367130]\n",
      "loss: 0.079476  [54400/367130]\n",
      "loss: 0.132022  [56000/367130]\n",
      "loss: 0.071467  [57600/367130]\n",
      "loss: 0.025280  [59200/367130]\n",
      "loss: 0.045498  [60800/367130]\n",
      "loss: 0.103372  [62400/367130]\n",
      "loss: 0.110170  [64000/367130]\n",
      "loss: 0.190496  [65600/367130]\n",
      "loss: 0.234765  [67200/367130]\n",
      "loss: 0.391274  [68800/367130]\n",
      "loss: 0.465812  [70400/367130]\n",
      "loss: 0.103494  [72000/367130]\n",
      "loss: 0.508370  [73600/367130]\n",
      "loss: 0.412500  [75200/367130]\n",
      "loss: 0.266051  [76800/367130]\n",
      "loss: 0.248740  [78400/367130]\n",
      "loss: 0.193272  [80000/367130]\n",
      "loss: 0.212503  [81600/367130]\n",
      "loss: 0.342148  [83200/367130]\n",
      "loss: 0.263038  [84800/367130]\n",
      "loss: 0.115467  [86400/367130]\n",
      "loss: 0.052953  [88000/367130]\n",
      "loss: 0.118169  [89600/367130]\n",
      "loss: 0.357028  [91200/367130]\n",
      "loss: 0.241560  [92800/367130]\n",
      "loss: 0.589874  [94400/367130]\n",
      "loss: 0.114396  [96000/367130]\n",
      "loss: 0.193761  [97600/367130]\n",
      "loss: 0.102441  [99200/367130]\n",
      "loss: 0.295059  [100800/367130]\n",
      "loss: 0.097318  [102400/367130]\n",
      "loss: 0.183462  [104000/367130]\n",
      "loss: 0.107350  [105600/367130]\n",
      "loss: 0.557643  [107200/367130]\n",
      "loss: 0.092826  [108800/367130]\n",
      "loss: 0.207177  [110400/367130]\n",
      "loss: 0.439741  [112000/367130]\n",
      "loss: 0.284411  [113600/367130]\n",
      "loss: 0.134315  [115200/367130]\n",
      "loss: 0.119626  [116800/367130]\n",
      "loss: 0.288606  [118400/367130]\n",
      "loss: 0.157368  [120000/367130]\n",
      "loss: 0.243382  [121600/367130]\n",
      "loss: 0.043402  [123200/367130]\n",
      "loss: 0.145388  [124800/367130]\n",
      "loss: 0.376052  [126400/367130]\n",
      "loss: 0.206171  [128000/367130]\n",
      "loss: 0.168852  [129600/367130]\n",
      "loss: 0.468215  [131200/367130]\n",
      "loss: 0.393856  [132800/367130]\n",
      "loss: 0.054635  [134400/367130]\n",
      "loss: 0.192615  [136000/367130]\n",
      "loss: 0.474436  [137600/367130]\n",
      "loss: 0.203592  [139200/367130]\n",
      "loss: 0.061699  [140800/367130]\n",
      "loss: 0.207534  [142400/367130]\n",
      "loss: 0.280247  [144000/367130]\n",
      "loss: 0.156228  [145600/367130]\n",
      "loss: 0.253780  [147200/367130]\n",
      "loss: 0.078982  [148800/367130]\n",
      "loss: 0.045753  [150400/367130]\n",
      "loss: 0.181110  [152000/367130]\n",
      "loss: 0.279886  [153600/367130]\n",
      "loss: 0.072405  [155200/367130]\n",
      "loss: 0.338699  [156800/367130]\n",
      "loss: 0.154739  [158400/367130]\n",
      "loss: 0.062107  [160000/367130]\n",
      "loss: 0.159254  [161600/367130]\n",
      "loss: 0.127099  [163200/367130]\n",
      "loss: 0.525081  [164800/367130]\n",
      "loss: 0.191414  [166400/367130]\n",
      "loss: 0.086939  [168000/367130]\n",
      "loss: 0.311529  [169600/367130]\n",
      "loss: 0.587358  [171200/367130]\n",
      "loss: 0.235321  [172800/367130]\n",
      "loss: 0.151761  [174400/367130]\n",
      "loss: 0.090736  [176000/367130]\n",
      "loss: 0.164052  [177600/367130]\n",
      "loss: 0.401004  [179200/367130]\n",
      "loss: 0.352337  [180800/367130]\n",
      "loss: 0.353061  [182400/367130]\n",
      "loss: 0.221469  [184000/367130]\n",
      "loss: 0.333209  [185600/367130]\n",
      "loss: 0.225509  [187200/367130]\n",
      "loss: 0.455472  [188800/367130]\n",
      "loss: 0.198186  [190400/367130]\n",
      "loss: 0.198543  [192000/367130]\n",
      "loss: 0.375342  [193600/367130]\n",
      "loss: 0.068144  [195200/367130]\n",
      "loss: 0.055999  [196800/367130]\n",
      "loss: 0.153536  [198400/367130]\n",
      "loss: 0.129225  [200000/367130]\n",
      "loss: 0.152712  [201600/367130]\n",
      "loss: 0.194697  [203200/367130]\n",
      "loss: 0.481127  [204800/367130]\n",
      "loss: 0.584768  [206400/367130]\n",
      "loss: 0.173902  [208000/367130]\n",
      "loss: 0.208371  [209600/367130]\n",
      "loss: 0.259355  [211200/367130]\n",
      "loss: 0.374448  [212800/367130]\n",
      "loss: 0.420984  [214400/367130]\n",
      "loss: 0.098969  [216000/367130]\n",
      "loss: 0.034686  [217600/367130]\n",
      "loss: 0.395106  [219200/367130]\n",
      "loss: 0.250081  [220800/367130]\n",
      "loss: 0.318320  [222400/367130]\n",
      "loss: 0.207869  [224000/367130]\n",
      "loss: 0.302146  [225600/367130]\n",
      "loss: 0.126979  [227200/367130]\n",
      "loss: 0.114763  [228800/367130]\n",
      "loss: 0.294447  [230400/367130]\n",
      "loss: 0.050766  [232000/367130]\n",
      "loss: 0.121083  [233600/367130]\n",
      "loss: 0.295659  [235200/367130]\n",
      "loss: 0.197624  [236800/367130]\n",
      "loss: 0.225228  [238400/367130]\n",
      "loss: 0.132403  [240000/367130]\n",
      "loss: 0.252255  [241600/367130]\n",
      "loss: 0.094413  [243200/367130]\n",
      "loss: 0.127482  [244800/367130]\n",
      "loss: 0.135514  [246400/367130]\n",
      "loss: 0.151864  [248000/367130]\n",
      "loss: 0.308300  [249600/367130]\n",
      "loss: 0.186544  [251200/367130]\n",
      "loss: 0.188434  [252800/367130]\n",
      "loss: 0.219817  [254400/367130]\n",
      "loss: 0.310183  [256000/367130]\n",
      "loss: 0.213434  [257600/367130]\n",
      "loss: 0.238556  [259200/367130]\n",
      "loss: 0.103655  [260800/367130]\n",
      "loss: 0.046797  [262400/367130]\n",
      "loss: 0.128056  [264000/367130]\n",
      "loss: 0.422208  [265600/367130]\n",
      "loss: 0.234878  [267200/367130]\n",
      "loss: 0.075730  [268800/367130]\n",
      "loss: 0.087946  [270400/367130]\n",
      "loss: 0.468916  [272000/367130]\n",
      "loss: 0.385010  [273600/367130]\n",
      "loss: 0.219604  [275200/367130]\n",
      "loss: 0.082423  [276800/367130]\n",
      "loss: 0.320297  [278400/367130]\n",
      "loss: 0.063760  [280000/367130]\n",
      "loss: 0.189347  [281600/367130]\n",
      "loss: 0.156470  [283200/367130]\n",
      "loss: 0.167101  [284800/367130]\n",
      "loss: 0.270358  [286400/367130]\n",
      "loss: 0.089903  [288000/367130]\n",
      "loss: 0.244160  [289600/367130]\n",
      "loss: 0.027673  [291200/367130]\n",
      "loss: 0.336969  [292800/367130]\n",
      "loss: 0.053020  [294400/367130]\n",
      "loss: 0.455762  [296000/367130]\n",
      "loss: 0.251553  [297600/367130]\n",
      "loss: 0.296207  [299200/367130]\n",
      "loss: 0.070916  [300800/367130]\n",
      "loss: 0.114825  [302400/367130]\n",
      "loss: 0.425453  [304000/367130]\n",
      "loss: 0.201981  [305600/367130]\n",
      "loss: 0.141050  [307200/367130]\n",
      "loss: 0.307845  [308800/367130]\n",
      "loss: 0.136237  [310400/367130]\n",
      "loss: 0.118662  [312000/367130]\n",
      "loss: 0.072572  [313600/367130]\n",
      "loss: 0.058275  [315200/367130]\n",
      "loss: 0.087311  [316800/367130]\n",
      "loss: 0.184336  [318400/367130]\n",
      "loss: 0.083982  [320000/367130]\n",
      "loss: 0.155167  [321600/367130]\n",
      "loss: 0.231498  [323200/367130]\n",
      "loss: 0.164620  [324800/367130]\n",
      "loss: 0.248035  [326400/367130]\n",
      "loss: 0.093237  [328000/367130]\n",
      "loss: 0.416712  [329600/367130]\n",
      "loss: 0.329835  [331200/367130]\n",
      "loss: 0.372830  [332800/367130]\n",
      "loss: 0.059316  [334400/367130]\n",
      "loss: 0.261855  [336000/367130]\n",
      "loss: 0.241496  [337600/367130]\n",
      "loss: 0.207455  [339200/367130]\n",
      "loss: 0.304514  [340800/367130]\n",
      "loss: 0.360925  [342400/367130]\n",
      "loss: 0.394768  [344000/367130]\n",
      "loss: 0.384227  [345600/367130]\n",
      "loss: 0.167781  [347200/367130]\n",
      "loss: 0.056177  [348800/367130]\n",
      "loss: 0.260617  [350400/367130]\n",
      "loss: 0.276958  [352000/367130]\n",
      "loss: 0.084871  [353600/367130]\n",
      "loss: 0.164963  [355200/367130]\n",
      "loss: 0.223768  [356800/367130]\n",
      "loss: 0.070362  [358400/367130]\n",
      "loss: 0.149452  [360000/367130]\n",
      "loss: 0.179259  [361600/367130]\n",
      "loss: 0.236433  [363200/367130]\n",
      "loss: 0.154869  [364800/367130]\n",
      "loss: 0.390813  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.223239 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.120622  [    0/367130]\n",
      "loss: 0.308326  [ 1600/367130]\n",
      "loss: 0.330146  [ 3200/367130]\n",
      "loss: 0.359382  [ 4800/367130]\n",
      "loss: 0.055921  [ 6400/367130]\n",
      "loss: 0.164108  [ 8000/367130]\n",
      "loss: 0.210425  [ 9600/367130]\n",
      "loss: 0.109813  [11200/367130]\n",
      "loss: 0.166680  [12800/367130]\n",
      "loss: 0.269316  [14400/367130]\n",
      "loss: 0.167186  [16000/367130]\n",
      "loss: 0.321150  [17600/367130]\n",
      "loss: 0.107017  [19200/367130]\n",
      "loss: 0.263231  [20800/367130]\n",
      "loss: 0.151895  [22400/367130]\n",
      "loss: 0.292552  [24000/367130]\n",
      "loss: 0.208370  [25600/367130]\n",
      "loss: 0.155549  [27200/367130]\n",
      "loss: 0.287293  [28800/367130]\n",
      "loss: 0.066603  [30400/367130]\n",
      "loss: 0.087063  [32000/367130]\n",
      "loss: 0.088019  [33600/367130]\n",
      "loss: 0.067779  [35200/367130]\n",
      "loss: 0.223833  [36800/367130]\n",
      "loss: 0.204220  [38400/367130]\n",
      "loss: 0.564776  [40000/367130]\n",
      "loss: 0.362649  [41600/367130]\n",
      "loss: 0.119413  [43200/367130]\n",
      "loss: 0.035775  [44800/367130]\n",
      "loss: 0.282985  [46400/367130]\n",
      "loss: 0.150487  [48000/367130]\n",
      "loss: 0.024625  [49600/367130]\n",
      "loss: 0.299556  [51200/367130]\n",
      "loss: 0.316655  [52800/367130]\n",
      "loss: 0.183242  [54400/367130]\n",
      "loss: 0.095400  [56000/367130]\n",
      "loss: 0.063847  [57600/367130]\n",
      "loss: 0.573414  [59200/367130]\n",
      "loss: 0.070763  [60800/367130]\n",
      "loss: 0.137148  [62400/367130]\n",
      "loss: 0.262005  [64000/367130]\n",
      "loss: 0.074674  [65600/367130]\n",
      "loss: 0.134056  [67200/367130]\n",
      "loss: 0.083060  [68800/367130]\n",
      "loss: 0.285834  [70400/367130]\n",
      "loss: 0.275283  [72000/367130]\n",
      "loss: 0.087626  [73600/367130]\n",
      "loss: 0.119926  [75200/367130]\n",
      "loss: 0.157428  [76800/367130]\n",
      "loss: 0.282512  [78400/367130]\n",
      "loss: 0.200427  [80000/367130]\n",
      "loss: 0.191360  [81600/367130]\n",
      "loss: 0.149510  [83200/367130]\n",
      "loss: 0.128988  [84800/367130]\n",
      "loss: 0.078698  [86400/367130]\n",
      "loss: 0.146684  [88000/367130]\n",
      "loss: 0.519233  [89600/367130]\n",
      "loss: 0.300499  [91200/367130]\n",
      "loss: 0.140991  [92800/367130]\n",
      "loss: 0.349892  [94400/367130]\n",
      "loss: 0.375725  [96000/367130]\n",
      "loss: 0.042985  [97600/367130]\n",
      "loss: 0.145137  [99200/367130]\n",
      "loss: 0.114090  [100800/367130]\n",
      "loss: 0.220068  [102400/367130]\n",
      "loss: 0.494799  [104000/367130]\n",
      "loss: 0.249260  [105600/367130]\n",
      "loss: 0.143078  [107200/367130]\n",
      "loss: 0.089699  [108800/367130]\n",
      "loss: 0.260967  [110400/367130]\n",
      "loss: 0.385149  [112000/367130]\n",
      "loss: 0.203048  [113600/367130]\n",
      "loss: 0.239289  [115200/367130]\n",
      "loss: 0.332681  [116800/367130]\n",
      "loss: 0.225050  [118400/367130]\n",
      "loss: 0.301342  [120000/367130]\n",
      "loss: 0.295376  [121600/367130]\n",
      "loss: 0.244020  [123200/367130]\n",
      "loss: 0.486464  [124800/367130]\n",
      "loss: 0.447023  [126400/367130]\n",
      "loss: 0.050626  [128000/367130]\n",
      "loss: 0.264854  [129600/367130]\n",
      "loss: 0.144360  [131200/367130]\n",
      "loss: 0.165058  [132800/367130]\n",
      "loss: 0.147626  [134400/367130]\n",
      "loss: 0.117071  [136000/367130]\n",
      "loss: 0.167968  [137600/367130]\n",
      "loss: 0.279617  [139200/367130]\n",
      "loss: 0.265425  [140800/367130]\n",
      "loss: 0.189438  [142400/367130]\n",
      "loss: 0.094625  [144000/367130]\n",
      "loss: 0.150745  [145600/367130]\n",
      "loss: 0.195618  [147200/367130]\n",
      "loss: 0.144260  [148800/367130]\n",
      "loss: 0.329171  [150400/367130]\n",
      "loss: 0.115773  [152000/367130]\n",
      "loss: 0.147570  [153600/367130]\n",
      "loss: 0.065204  [155200/367130]\n",
      "loss: 0.106225  [156800/367130]\n",
      "loss: 0.181158  [158400/367130]\n",
      "loss: 0.153484  [160000/367130]\n",
      "loss: 0.195339  [161600/367130]\n",
      "loss: 0.070115  [163200/367130]\n",
      "loss: 0.260537  [164800/367130]\n",
      "loss: 0.166277  [166400/367130]\n",
      "loss: 0.379450  [168000/367130]\n",
      "loss: 0.233115  [169600/367130]\n",
      "loss: 0.217598  [171200/367130]\n",
      "loss: 0.169230  [172800/367130]\n",
      "loss: 0.156819  [174400/367130]\n",
      "loss: 0.100926  [176000/367130]\n",
      "loss: 0.107594  [177600/367130]\n",
      "loss: 0.229774  [179200/367130]\n",
      "loss: 0.306534  [180800/367130]\n",
      "loss: 0.076377  [182400/367130]\n",
      "loss: 0.234094  [184000/367130]\n",
      "loss: 0.116786  [185600/367130]\n",
      "loss: 0.195663  [187200/367130]\n",
      "loss: 0.163623  [188800/367130]\n",
      "loss: 0.149318  [190400/367130]\n",
      "loss: 0.052349  [192000/367130]\n",
      "loss: 0.201572  [193600/367130]\n",
      "loss: 0.163051  [195200/367130]\n",
      "loss: 0.143764  [196800/367130]\n",
      "loss: 0.614261  [198400/367130]\n",
      "loss: 0.192224  [200000/367130]\n",
      "loss: 0.178270  [201600/367130]\n",
      "loss: 0.195444  [203200/367130]\n",
      "loss: 0.270448  [204800/367130]\n",
      "loss: 0.288199  [206400/367130]\n",
      "loss: 0.035727  [208000/367130]\n",
      "loss: 0.180192  [209600/367130]\n",
      "loss: 0.112555  [211200/367130]\n",
      "loss: 0.143335  [212800/367130]\n",
      "loss: 0.346389  [214400/367130]\n",
      "loss: 0.246317  [216000/367130]\n",
      "loss: 0.236075  [217600/367130]\n",
      "loss: 0.280582  [219200/367130]\n",
      "loss: 0.102104  [220800/367130]\n",
      "loss: 0.121478  [222400/367130]\n",
      "loss: 0.082731  [224000/367130]\n",
      "loss: 0.356926  [225600/367130]\n",
      "loss: 0.065195  [227200/367130]\n",
      "loss: 0.184410  [228800/367130]\n",
      "loss: 0.121531  [230400/367130]\n",
      "loss: 0.120403  [232000/367130]\n",
      "loss: 0.154585  [233600/367130]\n",
      "loss: 0.043003  [235200/367130]\n",
      "loss: 0.448623  [236800/367130]\n",
      "loss: 0.505662  [238400/367130]\n",
      "loss: 0.140079  [240000/367130]\n",
      "loss: 0.278149  [241600/367130]\n",
      "loss: 0.439552  [243200/367130]\n",
      "loss: 0.210074  [244800/367130]\n",
      "loss: 0.144528  [246400/367130]\n",
      "loss: 0.225108  [248000/367130]\n",
      "loss: 0.221781  [249600/367130]\n",
      "loss: 0.147082  [251200/367130]\n",
      "loss: 0.086458  [252800/367130]\n",
      "loss: 0.232060  [254400/367130]\n",
      "loss: 0.359556  [256000/367130]\n",
      "loss: 0.178509  [257600/367130]\n",
      "loss: 0.173352  [259200/367130]\n",
      "loss: 0.114011  [260800/367130]\n",
      "loss: 0.144864  [262400/367130]\n",
      "loss: 0.113708  [264000/367130]\n",
      "loss: 0.179602  [265600/367130]\n",
      "loss: 0.276864  [267200/367130]\n",
      "loss: 0.103331  [268800/367130]\n",
      "loss: 0.128012  [270400/367130]\n",
      "loss: 0.217106  [272000/367130]\n",
      "loss: 0.164088  [273600/367130]\n",
      "loss: 0.194894  [275200/367130]\n",
      "loss: 0.177860  [276800/367130]\n",
      "loss: 0.162209  [278400/367130]\n",
      "loss: 0.208713  [280000/367130]\n",
      "loss: 0.206458  [281600/367130]\n",
      "loss: 0.083130  [283200/367130]\n",
      "loss: 0.189646  [284800/367130]\n",
      "loss: 0.404493  [286400/367130]\n",
      "loss: 0.097836  [288000/367130]\n",
      "loss: 0.232261  [289600/367130]\n",
      "loss: 0.235636  [291200/367130]\n",
      "loss: 0.158004  [292800/367130]\n",
      "loss: 0.213352  [294400/367130]\n",
      "loss: 0.099920  [296000/367130]\n",
      "loss: 0.315643  [297600/367130]\n",
      "loss: 0.276589  [299200/367130]\n",
      "loss: 0.217291  [300800/367130]\n",
      "loss: 0.065949  [302400/367130]\n",
      "loss: 0.156555  [304000/367130]\n",
      "loss: 0.166342  [305600/367130]\n",
      "loss: 0.276801  [307200/367130]\n",
      "loss: 0.110320  [308800/367130]\n",
      "loss: 0.141238  [310400/367130]\n",
      "loss: 0.311409  [312000/367130]\n",
      "loss: 0.108338  [313600/367130]\n",
      "loss: 0.079240  [315200/367130]\n",
      "loss: 0.168729  [316800/367130]\n",
      "loss: 0.456794  [318400/367130]\n",
      "loss: 0.151684  [320000/367130]\n",
      "loss: 0.202282  [321600/367130]\n",
      "loss: 0.258846  [323200/367130]\n",
      "loss: 0.229540  [324800/367130]\n",
      "loss: 0.284144  [326400/367130]\n",
      "loss: 0.326774  [328000/367130]\n",
      "loss: 0.617901  [329600/367130]\n",
      "loss: 0.098624  [331200/367130]\n",
      "loss: 0.132857  [332800/367130]\n",
      "loss: 0.470279  [334400/367130]\n",
      "loss: 0.092638  [336000/367130]\n",
      "loss: 0.136472  [337600/367130]\n",
      "loss: 0.134727  [339200/367130]\n",
      "loss: 0.120964  [340800/367130]\n",
      "loss: 0.136552  [342400/367130]\n",
      "loss: 0.098003  [344000/367130]\n",
      "loss: 0.335790  [345600/367130]\n",
      "loss: 0.138560  [347200/367130]\n",
      "loss: 0.102985  [348800/367130]\n",
      "loss: 0.082383  [350400/367130]\n",
      "loss: 0.287374  [352000/367130]\n",
      "loss: 0.268900  [353600/367130]\n",
      "loss: 0.175713  [355200/367130]\n",
      "loss: 0.348952  [356800/367130]\n",
      "loss: 0.307290  [358400/367130]\n",
      "loss: 0.457875  [360000/367130]\n",
      "loss: 0.225798  [361600/367130]\n",
      "loss: 0.212093  [363200/367130]\n",
      "loss: 0.132572  [364800/367130]\n",
      "loss: 0.152985  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.222894 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.127015  [    0/367130]\n",
      "loss: 0.429906  [ 1600/367130]\n",
      "loss: 0.197883  [ 3200/367130]\n",
      "loss: 0.172313  [ 4800/367130]\n",
      "loss: 0.273767  [ 6400/367130]\n",
      "loss: 0.230048  [ 8000/367130]\n",
      "loss: 0.121375  [ 9600/367130]\n",
      "loss: 0.185892  [11200/367130]\n",
      "loss: 0.212106  [12800/367130]\n",
      "loss: 0.407360  [14400/367130]\n",
      "loss: 0.051179  [16000/367130]\n",
      "loss: 0.060306  [17600/367130]\n",
      "loss: 0.137991  [19200/367130]\n",
      "loss: 0.164343  [20800/367130]\n",
      "loss: 0.261660  [22400/367130]\n",
      "loss: 0.324346  [24000/367130]\n",
      "loss: 0.209797  [25600/367130]\n",
      "loss: 0.130831  [27200/367130]\n",
      "loss: 0.254515  [28800/367130]\n",
      "loss: 0.244168  [30400/367130]\n",
      "loss: 0.149721  [32000/367130]\n",
      "loss: 0.192207  [33600/367130]\n",
      "loss: 0.237394  [35200/367130]\n",
      "loss: 0.073408  [36800/367130]\n",
      "loss: 0.126703  [38400/367130]\n",
      "loss: 0.291771  [40000/367130]\n",
      "loss: 0.158889  [41600/367130]\n",
      "loss: 0.126890  [43200/367130]\n",
      "loss: 0.118915  [44800/367130]\n",
      "loss: 0.294792  [46400/367130]\n",
      "loss: 0.297538  [48000/367130]\n",
      "loss: 0.345842  [49600/367130]\n",
      "loss: 0.164861  [51200/367130]\n",
      "loss: 0.190697  [52800/367130]\n",
      "loss: 0.099937  [54400/367130]\n",
      "loss: 0.112539  [56000/367130]\n",
      "loss: 0.137231  [57600/367130]\n",
      "loss: 0.116580  [59200/367130]\n",
      "loss: 0.233895  [60800/367130]\n",
      "loss: 0.208200  [62400/367130]\n",
      "loss: 0.412867  [64000/367130]\n",
      "loss: 0.113126  [65600/367130]\n",
      "loss: 0.106388  [67200/367130]\n",
      "loss: 0.115041  [68800/367130]\n",
      "loss: 0.097779  [70400/367130]\n",
      "loss: 0.076887  [72000/367130]\n",
      "loss: 0.077086  [73600/367130]\n",
      "loss: 0.362082  [75200/367130]\n",
      "loss: 0.020264  [76800/367130]\n",
      "loss: 0.169317  [78400/367130]\n",
      "loss: 0.027830  [80000/367130]\n",
      "loss: 0.238963  [81600/367130]\n",
      "loss: 0.148148  [83200/367130]\n",
      "loss: 0.373405  [84800/367130]\n",
      "loss: 0.209273  [86400/367130]\n",
      "loss: 0.192200  [88000/367130]\n",
      "loss: 0.187787  [89600/367130]\n",
      "loss: 0.212230  [91200/367130]\n",
      "loss: 0.205181  [92800/367130]\n",
      "loss: 0.328213  [94400/367130]\n",
      "loss: 0.388471  [96000/367130]\n",
      "loss: 0.266134  [97600/367130]\n",
      "loss: 0.107861  [99200/367130]\n",
      "loss: 0.202848  [100800/367130]\n",
      "loss: 0.242244  [102400/367130]\n",
      "loss: 0.353454  [104000/367130]\n",
      "loss: 0.268726  [105600/367130]\n",
      "loss: 0.259106  [107200/367130]\n",
      "loss: 0.044653  [108800/367130]\n",
      "loss: 0.173280  [110400/367130]\n",
      "loss: 0.067454  [112000/367130]\n",
      "loss: 0.171671  [113600/367130]\n",
      "loss: 0.216555  [115200/367130]\n",
      "loss: 0.185009  [116800/367130]\n",
      "loss: 0.116786  [118400/367130]\n",
      "loss: 0.127225  [120000/367130]\n",
      "loss: 0.064790  [121600/367130]\n",
      "loss: 0.253469  [123200/367130]\n",
      "loss: 0.365193  [124800/367130]\n",
      "loss: 0.045898  [126400/367130]\n",
      "loss: 0.255664  [128000/367130]\n",
      "loss: 0.162484  [129600/367130]\n",
      "loss: 0.153476  [131200/367130]\n",
      "loss: 0.143876  [132800/367130]\n",
      "loss: 0.128823  [134400/367130]\n",
      "loss: 0.266671  [136000/367130]\n",
      "loss: 0.251708  [137600/367130]\n",
      "loss: 0.208714  [139200/367130]\n",
      "loss: 0.284727  [140800/367130]\n",
      "loss: 0.471955  [142400/367130]\n",
      "loss: 0.181820  [144000/367130]\n",
      "loss: 0.052862  [145600/367130]\n",
      "loss: 0.299609  [147200/367130]\n",
      "loss: 0.063264  [148800/367130]\n",
      "loss: 0.143157  [150400/367130]\n",
      "loss: 0.068019  [152000/367130]\n",
      "loss: 0.161029  [153600/367130]\n",
      "loss: 0.316225  [155200/367130]\n",
      "loss: 0.200200  [156800/367130]\n",
      "loss: 0.268423  [158400/367130]\n",
      "loss: 0.233023  [160000/367130]\n",
      "loss: 0.162697  [161600/367130]\n",
      "loss: 0.132095  [163200/367130]\n",
      "loss: 0.091401  [164800/367130]\n",
      "loss: 0.232981  [166400/367130]\n",
      "loss: 0.131842  [168000/367130]\n",
      "loss: 0.252798  [169600/367130]\n",
      "loss: 0.127126  [171200/367130]\n",
      "loss: 0.107739  [172800/367130]\n",
      "loss: 0.331614  [174400/367130]\n",
      "loss: 0.320209  [176000/367130]\n",
      "loss: 0.284412  [177600/367130]\n",
      "loss: 0.134330  [179200/367130]\n",
      "loss: 0.123831  [180800/367130]\n",
      "loss: 0.264121  [182400/367130]\n",
      "loss: 0.053870  [184000/367130]\n",
      "loss: 0.344291  [185600/367130]\n",
      "loss: 0.089944  [187200/367130]\n",
      "loss: 0.222148  [188800/367130]\n",
      "loss: 0.158606  [190400/367130]\n",
      "loss: 0.297082  [192000/367130]\n",
      "loss: 0.117279  [193600/367130]\n",
      "loss: 0.121739  [195200/367130]\n",
      "loss: 0.040954  [196800/367130]\n",
      "loss: 0.232356  [198400/367130]\n",
      "loss: 0.119871  [200000/367130]\n",
      "loss: 0.221543  [201600/367130]\n",
      "loss: 0.121769  [203200/367130]\n",
      "loss: 0.241253  [204800/367130]\n",
      "loss: 0.219649  [206400/367130]\n",
      "loss: 0.126263  [208000/367130]\n",
      "loss: 0.318166  [209600/367130]\n",
      "loss: 0.086437  [211200/367130]\n",
      "loss: 0.351736  [212800/367130]\n",
      "loss: 0.185090  [214400/367130]\n",
      "loss: 0.428986  [216000/367130]\n",
      "loss: 0.381954  [217600/367130]\n",
      "loss: 0.544237  [219200/367130]\n",
      "loss: 0.116779  [220800/367130]\n",
      "loss: 0.251812  [222400/367130]\n",
      "loss: 0.072029  [224000/367130]\n",
      "loss: 0.033240  [225600/367130]\n",
      "loss: 0.243278  [227200/367130]\n",
      "loss: 0.181992  [228800/367130]\n",
      "loss: 0.086932  [230400/367130]\n",
      "loss: 0.182042  [232000/367130]\n",
      "loss: 0.315266  [233600/367130]\n",
      "loss: 0.270934  [235200/367130]\n",
      "loss: 0.151203  [236800/367130]\n",
      "loss: 0.203475  [238400/367130]\n",
      "loss: 0.152740  [240000/367130]\n",
      "loss: 0.230918  [241600/367130]\n",
      "loss: 0.250854  [243200/367130]\n",
      "loss: 0.363189  [244800/367130]\n",
      "loss: 0.270251  [246400/367130]\n",
      "loss: 0.312839  [248000/367130]\n",
      "loss: 0.225259  [249600/367130]\n",
      "loss: 0.215943  [251200/367130]\n",
      "loss: 0.113898  [252800/367130]\n",
      "loss: 0.231085  [254400/367130]\n",
      "loss: 0.155038  [256000/367130]\n",
      "loss: 0.228849  [257600/367130]\n",
      "loss: 0.085000  [259200/367130]\n",
      "loss: 0.150631  [260800/367130]\n",
      "loss: 0.203078  [262400/367130]\n",
      "loss: 0.291384  [264000/367130]\n",
      "loss: 0.138570  [265600/367130]\n",
      "loss: 0.277664  [267200/367130]\n",
      "loss: 0.339614  [268800/367130]\n",
      "loss: 0.291321  [270400/367130]\n",
      "loss: 0.194212  [272000/367130]\n",
      "loss: 0.245748  [273600/367130]\n",
      "loss: 0.105898  [275200/367130]\n",
      "loss: 0.115419  [276800/367130]\n",
      "loss: 0.282976  [278400/367130]\n",
      "loss: 0.221570  [280000/367130]\n",
      "loss: 0.037394  [281600/367130]\n",
      "loss: 0.191242  [283200/367130]\n",
      "loss: 0.132699  [284800/367130]\n",
      "loss: 0.105980  [286400/367130]\n",
      "loss: 0.103955  [288000/367130]\n",
      "loss: 0.116070  [289600/367130]\n",
      "loss: 0.124299  [291200/367130]\n",
      "loss: 0.136597  [292800/367130]\n",
      "loss: 0.153069  [294400/367130]\n",
      "loss: 0.170194  [296000/367130]\n",
      "loss: 0.146210  [297600/367130]\n",
      "loss: 0.054950  [299200/367130]\n",
      "loss: 0.031252  [300800/367130]\n",
      "loss: 0.513018  [302400/367130]\n",
      "loss: 0.182838  [304000/367130]\n",
      "loss: 0.316230  [305600/367130]\n",
      "loss: 0.178865  [307200/367130]\n",
      "loss: 0.372354  [308800/367130]\n",
      "loss: 0.284654  [310400/367130]\n",
      "loss: 0.037049  [312000/367130]\n",
      "loss: 0.330989  [313600/367130]\n",
      "loss: 0.250675  [315200/367130]\n",
      "loss: 0.272679  [316800/367130]\n",
      "loss: 0.214835  [318400/367130]\n",
      "loss: 0.320456  [320000/367130]\n",
      "loss: 0.313644  [321600/367130]\n",
      "loss: 0.174236  [323200/367130]\n",
      "loss: 0.107708  [324800/367130]\n",
      "loss: 0.045692  [326400/367130]\n",
      "loss: 0.111032  [328000/367130]\n",
      "loss: 0.280827  [329600/367130]\n",
      "loss: 0.086438  [331200/367130]\n",
      "loss: 0.187160  [332800/367130]\n",
      "loss: 0.181654  [334400/367130]\n",
      "loss: 0.267392  [336000/367130]\n",
      "loss: 0.180375  [337600/367130]\n",
      "loss: 0.387368  [339200/367130]\n",
      "loss: 0.245751  [340800/367130]\n",
      "loss: 0.059503  [342400/367130]\n",
      "loss: 0.113446  [344000/367130]\n",
      "loss: 0.542602  [345600/367130]\n",
      "loss: 0.299846  [347200/367130]\n",
      "loss: 0.047771  [348800/367130]\n",
      "loss: 0.114209  [350400/367130]\n",
      "loss: 0.124991  [352000/367130]\n",
      "loss: 0.232383  [353600/367130]\n",
      "loss: 0.102108  [355200/367130]\n",
      "loss: 0.207739  [356800/367130]\n",
      "loss: 0.121897  [358400/367130]\n",
      "loss: 0.166826  [360000/367130]\n",
      "loss: 0.046852  [361600/367130]\n",
      "loss: 0.163144  [363200/367130]\n",
      "loss: 0.241681  [364800/367130]\n",
      "loss: 0.255290  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.223419 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.144090  [    0/367130]\n",
      "loss: 0.279147  [ 1600/367130]\n",
      "loss: 0.307473  [ 3200/367130]\n",
      "loss: 0.259925  [ 4800/367130]\n",
      "loss: 0.080560  [ 6400/367130]\n",
      "loss: 0.168839  [ 8000/367130]\n",
      "loss: 0.113372  [ 9600/367130]\n",
      "loss: 0.406962  [11200/367130]\n",
      "loss: 0.200065  [12800/367130]\n",
      "loss: 0.294022  [14400/367130]\n",
      "loss: 0.112675  [16000/367130]\n",
      "loss: 0.215294  [17600/367130]\n",
      "loss: 0.109255  [19200/367130]\n",
      "loss: 0.116751  [20800/367130]\n",
      "loss: 0.357132  [22400/367130]\n",
      "loss: 0.210903  [24000/367130]\n",
      "loss: 0.049821  [25600/367130]\n",
      "loss: 0.086772  [27200/367130]\n",
      "loss: 0.145192  [28800/367130]\n",
      "loss: 0.022006  [30400/367130]\n",
      "loss: 0.024988  [32000/367130]\n",
      "loss: 0.105078  [33600/367130]\n",
      "loss: 0.101157  [35200/367130]\n",
      "loss: 0.198447  [36800/367130]\n",
      "loss: 0.293939  [38400/367130]\n",
      "loss: 0.146318  [40000/367130]\n",
      "loss: 0.337064  [41600/367130]\n",
      "loss: 0.372133  [43200/367130]\n",
      "loss: 0.147109  [44800/367130]\n",
      "loss: 0.169484  [46400/367130]\n",
      "loss: 0.391789  [48000/367130]\n",
      "loss: 0.267673  [49600/367130]\n",
      "loss: 0.304843  [51200/367130]\n",
      "loss: 0.746211  [52800/367130]\n",
      "loss: 0.392491  [54400/367130]\n",
      "loss: 0.077095  [56000/367130]\n",
      "loss: 0.159728  [57600/367130]\n",
      "loss: 0.532183  [59200/367130]\n",
      "loss: 0.311085  [60800/367130]\n",
      "loss: 0.406753  [62400/367130]\n",
      "loss: 0.281837  [64000/367130]\n",
      "loss: 0.239187  [65600/367130]\n",
      "loss: 0.093697  [67200/367130]\n",
      "loss: 0.229044  [68800/367130]\n",
      "loss: 0.240968  [70400/367130]\n",
      "loss: 0.134843  [72000/367130]\n",
      "loss: 0.415417  [73600/367130]\n",
      "loss: 0.171814  [75200/367130]\n",
      "loss: 0.289741  [76800/367130]\n",
      "loss: 0.139394  [78400/367130]\n",
      "loss: 0.129378  [80000/367130]\n",
      "loss: 0.098668  [81600/367130]\n",
      "loss: 0.118420  [83200/367130]\n",
      "loss: 0.368838  [84800/367130]\n",
      "loss: 0.101932  [86400/367130]\n",
      "loss: 0.313841  [88000/367130]\n",
      "loss: 0.229490  [89600/367130]\n",
      "loss: 0.158846  [91200/367130]\n",
      "loss: 0.260321  [92800/367130]\n",
      "loss: 0.286406  [94400/367130]\n",
      "loss: 0.188056  [96000/367130]\n",
      "loss: 0.247890  [97600/367130]\n",
      "loss: 0.131972  [99200/367130]\n",
      "loss: 0.125979  [100800/367130]\n",
      "loss: 0.183832  [102400/367130]\n",
      "loss: 0.111320  [104000/367130]\n",
      "loss: 0.165248  [105600/367130]\n",
      "loss: 0.253911  [107200/367130]\n",
      "loss: 0.280032  [108800/367130]\n",
      "loss: 0.361366  [110400/367130]\n",
      "loss: 0.366874  [112000/367130]\n",
      "loss: 0.038610  [113600/367130]\n",
      "loss: 0.174746  [115200/367130]\n",
      "loss: 0.091758  [116800/367130]\n",
      "loss: 0.279333  [118400/367130]\n",
      "loss: 0.298019  [120000/367130]\n",
      "loss: 0.312148  [121600/367130]\n",
      "loss: 0.129882  [123200/367130]\n",
      "loss: 0.218926  [124800/367130]\n",
      "loss: 0.229342  [126400/367130]\n",
      "loss: 0.191994  [128000/367130]\n",
      "loss: 0.088260  [129600/367130]\n",
      "loss: 0.255723  [131200/367130]\n",
      "loss: 0.165533  [132800/367130]\n",
      "loss: 0.084579  [134400/367130]\n",
      "loss: 0.110785  [136000/367130]\n",
      "loss: 0.169130  [137600/367130]\n",
      "loss: 0.264414  [139200/367130]\n",
      "loss: 0.115498  [140800/367130]\n",
      "loss: 0.247663  [142400/367130]\n",
      "loss: 0.142788  [144000/367130]\n",
      "loss: 0.169688  [145600/367130]\n",
      "loss: 0.220634  [147200/367130]\n",
      "loss: 0.133883  [148800/367130]\n",
      "loss: 0.509514  [150400/367130]\n",
      "loss: 0.110264  [152000/367130]\n",
      "loss: 0.201836  [153600/367130]\n",
      "loss: 0.579652  [155200/367130]\n",
      "loss: 0.176621  [156800/367130]\n",
      "loss: 0.096732  [158400/367130]\n",
      "loss: 0.344100  [160000/367130]\n",
      "loss: 0.080428  [161600/367130]\n",
      "loss: 0.243748  [163200/367130]\n",
      "loss: 0.179022  [164800/367130]\n",
      "loss: 0.546268  [166400/367130]\n",
      "loss: 0.497383  [168000/367130]\n",
      "loss: 0.265264  [169600/367130]\n",
      "loss: 0.270462  [171200/367130]\n",
      "loss: 0.061657  [172800/367130]\n",
      "loss: 0.206835  [174400/367130]\n",
      "loss: 0.410685  [176000/367130]\n",
      "loss: 0.030287  [177600/367130]\n",
      "loss: 0.366030  [179200/367130]\n",
      "loss: 0.110929  [180800/367130]\n",
      "loss: 0.194779  [182400/367130]\n",
      "loss: 0.081776  [184000/367130]\n",
      "loss: 0.198168  [185600/367130]\n",
      "loss: 0.080353  [187200/367130]\n",
      "loss: 0.080109  [188800/367130]\n",
      "loss: 0.127848  [190400/367130]\n",
      "loss: 0.375035  [192000/367130]\n",
      "loss: 0.253448  [193600/367130]\n",
      "loss: 0.088442  [195200/367130]\n",
      "loss: 0.215367  [196800/367130]\n",
      "loss: 0.170088  [198400/367130]\n",
      "loss: 0.326642  [200000/367130]\n",
      "loss: 0.075348  [201600/367130]\n",
      "loss: 0.341791  [203200/367130]\n",
      "loss: 0.396362  [204800/367130]\n",
      "loss: 0.320428  [206400/367130]\n",
      "loss: 0.111907  [208000/367130]\n",
      "loss: 0.119951  [209600/367130]\n",
      "loss: 0.301370  [211200/367130]\n",
      "loss: 0.158609  [212800/367130]\n",
      "loss: 0.432439  [214400/367130]\n",
      "loss: 0.337210  [216000/367130]\n",
      "loss: 0.050015  [217600/367130]\n",
      "loss: 0.088166  [219200/367130]\n",
      "loss: 0.094597  [220800/367130]\n",
      "loss: 0.269488  [222400/367130]\n",
      "loss: 0.154406  [224000/367130]\n",
      "loss: 0.234372  [225600/367130]\n",
      "loss: 0.242879  [227200/367130]\n",
      "loss: 0.044834  [228800/367130]\n",
      "loss: 0.170807  [230400/367130]\n",
      "loss: 0.038916  [232000/367130]\n",
      "loss: 0.189895  [233600/367130]\n",
      "loss: 0.236339  [235200/367130]\n",
      "loss: 0.090958  [236800/367130]\n",
      "loss: 0.192904  [238400/367130]\n",
      "loss: 0.084072  [240000/367130]\n",
      "loss: 0.247986  [241600/367130]\n",
      "loss: 0.368810  [243200/367130]\n",
      "loss: 0.222961  [244800/367130]\n",
      "loss: 0.218172  [246400/367130]\n",
      "loss: 0.323660  [248000/367130]\n",
      "loss: 0.643885  [249600/367130]\n",
      "loss: 0.125282  [251200/367130]\n",
      "loss: 0.228836  [252800/367130]\n",
      "loss: 0.195152  [254400/367130]\n",
      "loss: 0.189300  [256000/367130]\n",
      "loss: 0.074576  [257600/367130]\n",
      "loss: 0.114304  [259200/367130]\n",
      "loss: 0.181831  [260800/367130]\n",
      "loss: 0.165874  [262400/367130]\n",
      "loss: 0.231017  [264000/367130]\n",
      "loss: 0.337529  [265600/367130]\n",
      "loss: 0.385082  [267200/367130]\n",
      "loss: 0.093957  [268800/367130]\n",
      "loss: 0.296048  [270400/367130]\n",
      "loss: 0.138601  [272000/367130]\n",
      "loss: 0.262454  [273600/367130]\n",
      "loss: 0.125624  [275200/367130]\n",
      "loss: 0.260283  [276800/367130]\n",
      "loss: 0.245213  [278400/367130]\n",
      "loss: 0.060064  [280000/367130]\n",
      "loss: 0.191776  [281600/367130]\n",
      "loss: 0.203982  [283200/367130]\n",
      "loss: 0.274499  [284800/367130]\n",
      "loss: 0.113452  [286400/367130]\n",
      "loss: 0.142040  [288000/367130]\n",
      "loss: 0.299336  [289600/367130]\n",
      "loss: 0.248397  [291200/367130]\n",
      "loss: 0.037996  [292800/367130]\n",
      "loss: 0.238511  [294400/367130]\n",
      "loss: 0.164360  [296000/367130]\n",
      "loss: 0.373308  [297600/367130]\n",
      "loss: 0.095452  [299200/367130]\n",
      "loss: 0.070891  [300800/367130]\n",
      "loss: 0.098541  [302400/367130]\n",
      "loss: 0.109846  [304000/367130]\n",
      "loss: 0.129920  [305600/367130]\n",
      "loss: 0.056647  [307200/367130]\n",
      "loss: 0.158613  [308800/367130]\n",
      "loss: 0.399933  [310400/367130]\n",
      "loss: 0.085056  [312000/367130]\n",
      "loss: 0.137271  [313600/367130]\n",
      "loss: 0.169533  [315200/367130]\n",
      "loss: 0.155870  [316800/367130]\n",
      "loss: 0.087687  [318400/367130]\n",
      "loss: 0.126437  [320000/367130]\n",
      "loss: 0.537631  [321600/367130]\n",
      "loss: 0.139404  [323200/367130]\n",
      "loss: 0.329494  [324800/367130]\n",
      "loss: 0.167568  [326400/367130]\n",
      "loss: 0.260274  [328000/367130]\n",
      "loss: 0.088208  [329600/367130]\n",
      "loss: 0.143554  [331200/367130]\n",
      "loss: 0.310991  [332800/367130]\n",
      "loss: 0.246979  [334400/367130]\n",
      "loss: 0.263545  [336000/367130]\n",
      "loss: 0.064619  [337600/367130]\n",
      "loss: 0.087752  [339200/367130]\n",
      "loss: 0.362711  [340800/367130]\n",
      "loss: 0.169184  [342400/367130]\n",
      "loss: 0.144425  [344000/367130]\n",
      "loss: 0.209760  [345600/367130]\n",
      "loss: 0.425390  [347200/367130]\n",
      "loss: 0.149076  [348800/367130]\n",
      "loss: 0.415416  [350400/367130]\n",
      "loss: 0.155324  [352000/367130]\n",
      "loss: 0.152019  [353600/367130]\n",
      "loss: 0.279724  [355200/367130]\n",
      "loss: 0.063312  [356800/367130]\n",
      "loss: 0.200173  [358400/367130]\n",
      "loss: 0.067881  [360000/367130]\n",
      "loss: 0.145402  [361600/367130]\n",
      "loss: 0.103008  [363200/367130]\n",
      "loss: 0.139003  [364800/367130]\n",
      "loss: 0.235484  [366400/367130]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.223225 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.126952  [    0/367130]\n",
      "loss: 0.341682  [ 1600/367130]\n",
      "loss: 0.106126  [ 3200/367130]\n",
      "loss: 0.313443  [ 4800/367130]\n",
      "loss: 0.377849  [ 6400/367130]\n",
      "loss: 0.182688  [ 8000/367130]\n",
      "loss: 0.041242  [ 9600/367130]\n",
      "loss: 0.129592  [11200/367130]\n",
      "loss: 0.252281  [12800/367130]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: lr_lambda)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, cycle_momentum=False)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    models[t] = model\n",
    "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "    scheduler.step()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(models[1], \"./model\")\n",
    "#model = torch.load(\"./model\")\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = models[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmericanExpressProfileTimeSeriesValidationDataset(Dataset):\n",
    "    def __init__(self, dataset_file, transformation=False):\n",
    "        self.dataset = pd.read_csv(dataset_file, nrows=10)\n",
    "        self.transformation = transformation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.dataset.iloc[[idx]]\n",
    "        data = row.drop(['customer_ID'], axis=1)\n",
    "\n",
    "        data = data.values[0].tolist()\n",
    "\n",
    "        for idx, value in enumerate(data):\n",
    "            if idx == 0: continue\n",
    "            list_ = value[1:-1].split(\", \")\n",
    "            for idx_l, elem in enumerate(list_):\n",
    "                list_[idx_l] = float(elem)\n",
    "            data[idx] = list_\n",
    "        data = data[1:]\n",
    "        \n",
    "        \n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        data = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        data = data.to(device)\n",
    "        if self.transformation: data = self.transformation(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_validation_dataset = AmericanExpressProfileTimeSeriesValidationDataset(\"transformed_test_dataset\", transformation=lambda data: data.T)\n",
    "\n",
    "validation_dataloader = DataLoader(full_validation_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_validation_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(dataloader, model, out_file):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data_file = open(f\"{out_file}.csv\", \"w\")\n",
    "        data_file.write(\"customer_ID,prediction\\n\")\n",
    "        for X, customer_ID in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.tolist()\n",
    "            for idx in range(len(customer_ID)):\n",
    "                data_file.write(customer_ID[idx] + \",\" + str(pred[idx])+\"\\n\")\n",
    "        data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loop(validation_dataloader, model, \"./test_data/test_labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
