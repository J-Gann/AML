@article{dorogush2018catboost,
  title={CatBoost: gradient boosting with categorical features support},
  author={Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
  journal={arXiv preprint arXiv:1810.11363},
  year={2018}
}

% To support an introduction claim that selling to debt collector is bad
@article{beck2017determines,
  title={What determines collection rates of debt collection agencies?},
  author={Beck, Timo and Grunert, Jens and Neus, Werner and Walter, Andreas},
  journal={Financial Review},
  volume={52},
  number={2},
  pages={259--279},
  year={2017},
  publisher={Wiley Online Library}
}


% To support the claim that Gini is closely related to ROC.
@article{schechtman2019relationship,
  title={The relationship between Gini terminology and the ROC curve},
  author={Schechtman, Edna and Schechtman, Gideon},
  journal={Metron},
  volume={77},
  number={3},
  pages={171--178},
  year={2019},
  publisher={Springer}
}

% Use to back up the claim that GB decision tree is typically recommended in
% DL (introduction). Also use as prior work later!
@article{shwartz2022tabular,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}

% Feature Importance using Random forest paper.
@article{lundberg2020local,
  title={From local explanations to global understanding with explainable AI for trees},
  author={Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  journal={Nature machine intelligence},
  volume={2},
  number={1},
  pages={56--67},
  year={2020},
  publisher={Nature Publishing Group}
}

% Optuna paper Hyperparam
@inproceedings{akiba2019optuna,
  title={Optuna: A next-generation hyperparameter optimization framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2623--2631},
  year={2019}
}

% Paper DNN architekturen für time series
% Wurde in Project Proposals zitiert, also vielleicht auch hier.
@inproceedings{katarya2018study,
  title={A study on neural networks approach to time-series analysis},
  author={Katarya, Rahul and Rastogi, Shubham},
  booktitle={2018 2nd International Conference on Inventive Systems and Control (ICISC)},
  pages={116--119},
  year={2018},
  organization={IEEE}
}

% A reasoning why statistical correlation between input features may be bad.
@article{yoo2014study,
  title={A study of effects of multicollinearity in the multivariable analysis},
  author={Yoo, Wonsuk and Mayberry, Robert and Bae, Sejong and Singh, Karan and He, Qinghua Peter and Lillard Jr, James W},
  journal={International journal of applied science and technology},
  volume={4},
  number={5},
  pages={9},
  year={2014},
  publisher={NIH Public Access}
}

% Related Work: Early Credit Default Models: Merton
@article{merton1974pricing,
  title={On the pricing of corporate debt: The risk structure of interest rates},
  author={Merton, Robert C},
  journal={The Journal of finance},
  volume={29},
  number={2},
  pages={449--470},
  year={1974},
  publisher={JSTOR}
}

% Related Work: Statistical Classification for credit scoring
@article{hand1997statistical,
  title={Statistical classification methods in consumer credit scoring: a review},
  author={Hand, David J and Henley, William E},
  journal={Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume={160},
  number={3},
  pages={523--541},
  year={1997},
  publisher={Wiley Online Library}
}

% Related Work: Credit Default mit SVM
@article{trustorff2011credit,
  title={Credit risk prediction using support vector machines},
  author={Trustorff, Jan-Henning and Konrad, Paul Markus and Leker, Jens},
  journal={Review of Quantitative Finance and Accounting},
  volume={36},
  number={4},
  pages={565--581},
  year={2011},
  publisher={Springer}
}

% Related Work: Nochmal SVM
@article{moula2017credit,
  title={Credit default prediction modeling: an application of support vector machine},
  author={Moula, Fahmida E and Guotai, Chi and Abedin, Mohammad Zoynul},
  journal={Risk Management},
  volume={19},
  number={2},
  pages={158--187},
  year={2017},
  publisher={Springer}
}

% Related Work: Random Forest und viel verschiedene modelle und datensätze
@article{alam2020investigation,
  title={An investigation of credit card default prediction in the imbalanced datasets},
  author={Alam, Talha Mahboob and Shaukat, Kamran and Hameed, Ibrahim A and Luo, Suhuai and Sarwar, Muhammad Umer and Shabbir, Shakir and Li, Jiaming and Khushi, Matloob},
  journal={IEEE Access},
  volume={8},
  pages={201173--201198},
  year={2020},
  publisher={IEEE}
}

% RElated Work: GCN für credit default prediction, sowie gute related work diskussion
@article{lee2021graph,
  title={Graph convolutional network-based credit default prediction utilizing three types of virtual distances among borrowers},
  author={Lee, Jong Wook and Lee, Won Kyung and Sohn, So Young},
  journal={Expert Systems with Applications},
  volume={168},
  pages={114411},
  year={2021},
  publisher={Elsevier}
}

% Vielleicht in Bezug auf Conclusion: Why are we using black box machine learninh when we don't need to? Explainable AI Perspektive
@article{Rudin2019Why,
	author = {Rudin, Cynthia and Radin, Joanna},
	journal = {Harvard Data Science Review},
	number = {2},
	year = {2019},
	month = {nov 22},
	note = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
	publisher = {},
	title = {Why {Are} {We} {Using} {Black} {Box} {Models} in {AI} {When} {We} {Don}\textquoteright{}t {Need} {To}? {A} {Lesson} {From} an {Explainable} {AI} {Competition}},
	volume = {1},
}

% related work: Focal loss für class imbalance. Hmm kommt eigentlich aus deep vision
@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

% the curse of imbalanced datasets
@inproceedings{kubat1997addressing,
  title={Addressing the curse of imbalanced training sets: one-sided selection},
  author={Kubat, Miroslav and Matwin, Stan and others},
  booktitle={Icml},
  volume={97},
  number={1},
  pages={179},
  year={1997},
  organization={Citeseer}
}

% Literaturrecherche: Das ähnlichste zu dem was wir machen, was ich gefunden habe
% Insbesondere ist interessant: feature importance
% Desweiteren: Evidenz das DL vielleicht gar nicht mal so gut ist.
% Coole ROC-AUC analyse mit gegenüberstellungen.
@article{addo2018credit,
  title={Credit risk analysis using machine and deep learning models},
  author={Addo, Peter Martey and Guegan, Dominique and Hassani, Bertrand},
  journal={Risks},
  volume={6},
  number={2},
  pages={38},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}

% ChallengeR für finale Analyse?!
@article{wiesenfarth2021methods,
  title={Methods and open-source toolkit for analyzing and visualizing challenge results},
  author={Wiesenfarth, Manuel and Reinke, Annika and Landman, Bennett A and Eisenmann, Matthias and Saiz, Laura Aguilera and Cardoso, M Jorge and Maier-Hein, Lena and Kopp-Schneider, Annette},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--15},
  year={2021},
  publisher={Nature Publishing Group}
}

% Wilcoxon signed rank test citation
@book{conover1999practical,
  title={Practical nonparametric statistics},
  author={Conover, William Jay},
  volume={350},
  year={1999},
  publisher={john wiley \& sons}
}

% Diskussion: Overconfidence in Neural Network approaches.
@article{abdar2021review,
  title={A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
  author={Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U Rajendra and others},
  journal={Information Fusion},
  volume={76},
  pages={243--297},
  year={2021},
  publisher={Elsevier}
}


@InProceedings{pmlr-v32-hutter14,
  title = 	 {An Efficient Approach for Assessing Hyperparameter Importance},
  author = 	 {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {754--762},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/hutter14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/hutter14.html},
  abstract = 	 {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very high-dimensional cases—most performance variation is attributable to just a few hyperparameters.}
}

% Catboost paper
@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}



@article{DBLP:journals/corr/abs-1809-04356,
  author    = {Hassan Ismail Fawaz and
               Germain Forestier and
               Jonathan Weber and
               Lhassane Idoumghar and
               Pierre{-}Alain Muller},
  title     = {Deep learning for time series classification: a review},
  journal   = {CoRR},
  volume    = {abs/1809.04356},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.04356},
  eprinttype = {arXiv},
  eprint    = {1809.04356},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-04356.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/IoffeS15,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03167},
  eprinttype = {arXiv},
  eprint    = {1502.03167},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
