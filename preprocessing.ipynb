{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/AML/preprocessing.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656e6269416d6f734f776e227d/home/ubuntu/AML/preprocessing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656e6269416d6f734f776e227d/home/ubuntu/AML/preprocessing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m pd\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mchained_assignment \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# default='warn'\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656e6269416d6f734f776e227d/home/ubuntu/AML/preprocessing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillSeries(row):\n",
    "    new_row = []\n",
    "    size = len(row[1])\n",
    "    if size < 13:\n",
    "        for idx in range(len(row)):\n",
    "            if idx == 0: new_row.append(row[idx])\n",
    "            else: new_row.append([row[idx][0] for cnt in range(13 - size)] + row[idx])\n",
    "    else:\n",
    "        new_row = row\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train_data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path, label_path=False):\n",
    "    dataset = pd.read_csv(data_path)\n",
    "    if label_path: labels = pd.read_csv(label_path)\n",
    "\n",
    "\n",
    "    categorical_ = dataset[['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']]\n",
    "\n",
    "    categorical_['D_63'] = categorical_['D_63'].map({\"CR\": 1, \"CO\": 2, \"CL\": 3, \"XZ\": 4, \"XM\": 5, \"XL\": 6})\n",
    "    categorical_['D_64'] = categorical_['D_64'].map({\"O\": 1, \"R\": 2, \"nan\": pd.NA, \"U\": 3, \"-1\": 4})\n",
    "\n",
    "    categorical_ = categorical_.fillna(0)\n",
    "\n",
    "\n",
    "    categorical_.astype('int16')\n",
    "\n",
    "\n",
    "    dataset_ = dataset.drop(['customer_ID', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'], axis=1)\n",
    "\n",
    "\n",
    "    timeint = pd.to_datetime(dataset['S_2']).astype(int)\n",
    "    timeint = (timeint-timeint.min())/(timeint.max()-timeint.min())\n",
    "    dataset_['S_2'] = timeint\n",
    "\n",
    "    dataset_ = dataset_.fillna(0)\n",
    "\n",
    "    dataset_.astype('float16')\n",
    "\n",
    "    dataset_final = pd.concat([dataset[['customer_ID']], dataset_, categorical_], axis=1)\n",
    "\n",
    "    dataset_final = dataset_final.groupby([\"customer_ID\"]).agg(list)\n",
    "    if label_path: dataset_final = dataset_final.join(labels.set_index('customer_ID'))\n",
    "    dataset_final = dataset_final.reset_index()\n",
    "    return dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(file_path, out_path, customers_per_file):\n",
    "    # split the training dataset into smaller chunks\n",
    "    customers_per_file = customers_per_file + 1\n",
    "    ids = {}\n",
    "    cnt = 0\n",
    "    ids_file = open('{}/{}.csv'.format(out_path, \"ids\"), \"w\")\n",
    "    ids_file.write(\"id,chunk\\n\")\n",
    "    save = False\n",
    "    with open(file_path) as bigfile:\n",
    "        for lineno, line in enumerate(bigfile):\n",
    "            if lineno == 0: continue    # skip header\n",
    "            customer_ID = line.split(\",\")[0]    # get customer ID of current line\n",
    "\n",
    "            sum_ids = len(ids.keys())\n",
    "\n",
    "            # add customer to list of read customers\n",
    "            if customer_ID in ids: ids[customer_ID].append(line)\n",
    "            else:\n",
    "                if sum_ids != (customers_per_file - 1): ids[customer_ID] = [line]\n",
    "                else: save = True\n",
    "\n",
    "\n",
    "            # write data of all read customers to a new file\n",
    "            if save:\n",
    "                data_file = open(f\"{out_path}/data{cnt}.csv\", \"w\")\n",
    "                data_file.write(\"customer_ID,S_2,P_2,D_39,B_1,B_2,R_1,S_3,D_41,B_3,D_42,D_43,D_44,B_4,D_45,B_5,R_2,D_46,D_47,D_48,D_49,B_6,B_7,B_8,D_50,D_51,B_9,R_3,D_52,P_3,B_10,D_53,S_5,B_11,S_6,D_54,R_4,S_7,B_12,S_8,D_55,D_56,B_13,R_5,D_58,S_9,B_14,D_59,D_60,D_61,B_15,S_11,D_62,D_63,D_64,D_65,B_16,B_17,B_18,B_19,D_66,B_20,D_68,S_12,R_6,S_13,B_21,D_69,B_22,D_70,D_71,D_72,S_15,B_23,D_73,P_4,D_74,D_75,D_76,B_24,R_7,D_77,B_25,B_26,D_78,D_79,R_8,R_9,S_16,D_80,R_10,R_11,B_27,D_81,D_82,S_17,R_12,B_28,R_13,D_83,R_14,R_15,D_84,R_16,B_29,B_30,S_18,D_86,D_87,R_17,R_18,D_88,B_31,S_19,R_19,B_32,S_20,R_20,R_21,B_33,D_89,R_22,R_23,D_91,D_92,D_93,D_94,R_24,R_25,D_96,S_22,S_23,S_24,S_25,S_26,D_102,D_103,D_104,D_105,D_106,D_107,B_36,B_37,R_26,R_27,B_38,D_108,D_109,D_110,D_111,B_39,D_112,B_40,S_27,D_113,D_114,D_115,D_116,D_117,D_118,D_119,D_120,D_121,D_122,D_123,D_124,D_125,D_126,D_127,D_128,D_129,B_41,B_42,D_130,D_131,D_132,D_133,R_28,D_134,D_135,D_136,D_137,D_138,D_139,D_140,D_141,D_142,D_143,D_144,D_145\\n\")\n",
    "                \n",
    "                for key in ids:\n",
    "                    data = ids[key]\n",
    "                    # copy first element of series until series length is 13\n",
    "                    if len(data) != 13: data = [data[0] for cnt in range(13 - len(data) )] + data \n",
    "                    ids_file.write(key + \",\" + str(cnt) + \"\\n\")\n",
    "                    for elem in data: data_file.write(elem)\n",
    "\n",
    "                data_file.close()\n",
    "                ids = {}\n",
    "                ids[customer_ID] = [line]\n",
    "                save = False\n",
    "                cnt += 1\n",
    "\n",
    "    data_file = open(f\"{out_path}/data{cnt}.csv\", \"w\")\n",
    "    data_file.write(\"customer_ID,S_2,P_2,D_39,B_1,B_2,R_1,S_3,D_41,B_3,D_42,D_43,D_44,B_4,D_45,B_5,R_2,D_46,D_47,D_48,D_49,B_6,B_7,B_8,D_50,D_51,B_9,R_3,D_52,P_3,B_10,D_53,S_5,B_11,S_6,D_54,R_4,S_7,B_12,S_8,D_55,D_56,B_13,R_5,D_58,S_9,B_14,D_59,D_60,D_61,B_15,S_11,D_62,D_63,D_64,D_65,B_16,B_17,B_18,B_19,D_66,B_20,D_68,S_12,R_6,S_13,B_21,D_69,B_22,D_70,D_71,D_72,S_15,B_23,D_73,P_4,D_74,D_75,D_76,B_24,R_7,D_77,B_25,B_26,D_78,D_79,R_8,R_9,S_16,D_80,R_10,R_11,B_27,D_81,D_82,S_17,R_12,B_28,R_13,D_83,R_14,R_15,D_84,R_16,B_29,B_30,S_18,D_86,D_87,R_17,R_18,D_88,B_31,S_19,R_19,B_32,S_20,R_20,R_21,B_33,D_89,R_22,R_23,D_91,D_92,D_93,D_94,R_24,R_25,D_96,S_22,S_23,S_24,S_25,S_26,D_102,D_103,D_104,D_105,D_106,D_107,B_36,B_37,R_26,R_27,B_38,D_108,D_109,D_110,D_111,B_39,D_112,B_40,S_27,D_113,D_114,D_115,D_116,D_117,D_118,D_119,D_120,D_121,D_122,D_123,D_124,D_125,D_126,D_127,D_128,D_129,B_41,B_42,D_130,D_131,D_132,D_133,R_28,D_134,D_135,D_136,D_137,D_138,D_139,D_140,D_141,D_142,D_143,D_144,D_145\\n\")\n",
    "                \n",
    "    for key in ids:\n",
    "        data = ids[key]\n",
    "        # copy first element of series until series length is 13\n",
    "        if len(data) != 13: data = [data[0] for cnt in range(13 - len(data) )] + data\n",
    "        ids_file.write(key + \",\" + str(cnt) + \"\\n\")\n",
    "        for elem in data: data_file.write(elem)\n",
    "\n",
    "    data_file.close()\n",
    "    ids = {}\n",
    "    cnt += 1\n",
    "\n",
    "    ids_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_file(\"train_data/train_data.csv\", \"train_data/train_data_chunks/\", 10000)\n",
    "#split_file(\"test_data/test_data.csv\", \"test_data/test_data_chunks/\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunks(folder_path, label_path=False):    \n",
    "    dfs = []\n",
    "    for file in sorted(os.listdir(folder_path)):\n",
    "        if file == \"ids.csv\": continue\n",
    "        print(file)\n",
    "        df = preprocess(os.path.join(folder_path, file), label_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data0.csv\n",
      "data1.csv\n",
      "data10.csv\n",
      "data11.csv\n",
      "data12.csv\n",
      "data13.csv\n",
      "data14.csv\n",
      "data15.csv\n",
      "data16.csv\n",
      "data17.csv\n",
      "data18.csv\n",
      "data19.csv\n",
      "data2.csv\n",
      "data20.csv\n",
      "data21.csv\n",
      "data22.csv\n",
      "data23.csv\n",
      "data24.csv\n",
      "data25.csv\n",
      "data26.csv\n",
      "data27.csv\n",
      "data28.csv\n",
      "data29.csv\n",
      "data3.csv\n",
      "data30.csv\n",
      "data31.csv\n",
      "data32.csv\n",
      "data33.csv\n",
      "data34.csv\n",
      "data35.csv\n",
      "data36.csv\n",
      "data37.csv\n",
      "data38.csv\n",
      "data39.csv\n",
      "data4.csv\n",
      "data40.csv\n",
      "data41.csv\n",
      "data42.csv\n",
      "data43.csv\n",
      "data44.csv\n",
      "data45.csv\n",
      "data46.csv\n",
      "data47.csv\n",
      "data48.csv\n",
      "data49.csv\n",
      "data5.csv\n",
      "data50.csv\n",
      "data51.csv\n",
      "data52.csv\n",
      "data53.csv\n",
      "data54.csv\n",
      "data55.csv\n",
      "data56.csv\n",
      "data57.csv\n",
      "data58.csv\n",
      "data59.csv\n",
      "data6.csv\n",
      "data60.csv\n",
      "data61.csv\n",
      "data62.csv\n",
      "data63.csv\n",
      "data64.csv\n",
      "data65.csv\n",
      "data66.csv\n",
      "data67.csv\n",
      "data68.csv\n",
      "data69.csv\n",
      "data7.csv\n",
      "data70.csv\n",
      "data71.csv\n",
      "data72.csv\n",
      "data73.csv\n",
      "data74.csv\n",
      "data75.csv\n",
      "data76.csv\n",
      "data77.csv\n",
      "data78.csv\n",
      "data79.csv\n",
      "data8.csv\n",
      "data80.csv\n",
      "data81.csv\n",
      "data82.csv\n",
      "data83.csv\n",
      "data84.csv\n",
      "data85.csv\n",
      "data86.csv\n",
      "data87.csv\n",
      "data88.csv\n",
      "data89.csv\n",
      "data9.csv\n",
      "data90.csv\n",
      "data91.csv\n",
      "data92.csv\n"
     ]
    }
   ],
   "source": [
    "#res = preprocess_chunks(\"train_data/train_data_chunks/\", \"train_data/train_labels.csv\")\n",
    "res = preprocess_chunks(\"test_data/test_data_chunks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res.to_csv(\"./transformed_dataset.csv\", chunksize=100)\n",
    "res.to_csv(\"./transformed_test_dataset\", chunksize=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
